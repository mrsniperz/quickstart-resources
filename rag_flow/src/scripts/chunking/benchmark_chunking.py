#!/usr/bin/env python3
"""
æ¨¡å—åç§°: benchmark_chunking
åŠŸèƒ½æè¿°: ç®€åŒ–åˆ†å—ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬
åˆ›å»ºæ—¥æœŸ: 2024-01-15
ä½œè€…: Sniperz
ç‰ˆæœ¬: v2.0.0

ä½¿ç”¨è¯´æ˜:
    python benchmark_chunking.py                    # è¿è¡Œæ ‡å‡†åŸºå‡†æµ‹è¯•
    python benchmark_chunking.py --preset semantic  # æµ‹è¯•ç‰¹å®šé¢„è®¾
    python benchmark_chunking.py --sizes 1000 5000 10000  # è‡ªå®šä¹‰æµ‹è¯•å¤§å°
    python benchmark_chunking.py --iterations 10    # è®¾ç½®æµ‹è¯•è¿­ä»£æ¬¡æ•°
    python benchmark_chunking.py --output results.json  # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
"""

import argparse
import json
import time
import statistics
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

try:
    from core.document_processor.chunking.chunking_engine import ChunkingEngine
    CHUNKING_ENGINE_AVAILABLE = True
except ImportError as e:
    print(f"å¯¼å…¥ChunkingEngineå¤±è´¥: {e}")
    CHUNKING_ENGINE_AVAILABLE = False


class ChunkingBenchmark:
    """åˆ†å—ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åŸºå‡†æµ‹è¯•å™¨"""
        self.engine = None
        if CHUNKING_ENGINE_AVAILABLE:
            try:
                self.engine = ChunkingEngine()
                print("âœ… åˆ†å—å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âŒ åˆ†å—å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
        else:
            print("âŒ åˆ†å—å¼•æ“ä¸å¯ç”¨")
    
    def generate_test_text(self, size: int) -> str:
        """
        ç”ŸæˆæŒ‡å®šå¤§å°çš„æµ‹è¯•æ–‡æœ¬
        
        Args:
            size: ç›®æ ‡æ–‡æœ¬å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            
        Returns:
            str: ç”Ÿæˆçš„æµ‹è¯•æ–‡æœ¬
        """
        base_text = """
ç¬¬ä¸€ç«  ç³»ç»Ÿæ¶æ„è®¾è®¡

æœ¬ç« ä»‹ç»ç³»ç»Ÿçš„æ•´ä½“æ¶æ„è®¾è®¡ç†å¿µå’Œå®ç°æ–¹æ¡ˆã€‚ç³»ç»Ÿé‡‡ç”¨å¾®æœåŠ¡æ¶æ„ï¼Œå…·æœ‰é«˜å¯ç”¨æ€§ã€å¯æ‰©å±•æ€§å’Œå¯ç»´æŠ¤æ€§çš„ç‰¹ç‚¹ã€‚

1.1 æ¶æ„æ¦‚è¿°
å¾®æœåŠ¡æ¶æ„æ˜¯ä¸€ç§å°†å•ä¸€åº”ç”¨ç¨‹åºå¼€å‘ä¸ºä¸€ç»„å°å‹æœåŠ¡çš„æ–¹æ³•ï¼Œæ¯ä¸ªæœåŠ¡è¿è¡Œåœ¨è‡ªå·±çš„è¿›ç¨‹ä¸­ï¼Œå¹¶ä½¿ç”¨è½»é‡çº§æœºåˆ¶ï¼ˆé€šå¸¸æ˜¯HTTPèµ„æºAPIï¼‰è¿›è¡Œé€šä¿¡ã€‚è¿™äº›æœåŠ¡å›´ç»•ä¸šåŠ¡åŠŸèƒ½æ„å»ºï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡å…¨è‡ªåŠ¨éƒ¨ç½²æœºåˆ¶ç‹¬ç«‹éƒ¨ç½²ã€‚

1.2 æ ¸å¿ƒç»„ä»¶
ç³»ç»Ÿä¸»è¦åŒ…å«ä»¥ä¸‹æ ¸å¿ƒç»„ä»¶ï¼š
- ç”¨æˆ·ç®¡ç†æœåŠ¡ï¼šè´Ÿè´£ç”¨æˆ·è®¤è¯ã€æˆæƒå’Œç”¨æˆ·ä¿¡æ¯ç®¡ç†
- æ•°æ®å¤„ç†æœåŠ¡ï¼šå¤„ç†å„ç§æ•°æ®çš„é‡‡é›†ã€æ¸…æ´—å’Œåˆ†æ
- æ¥å£ç½‘å…³ï¼šç»Ÿä¸€çš„APIå…¥å£ï¼Œè´Ÿè´£è·¯ç”±ã€é™æµå’Œå®‰å…¨æ§åˆ¶
- é…ç½®ä¸­å¿ƒï¼šé›†ä¸­ç®¡ç†å„æœåŠ¡çš„é…ç½®ä¿¡æ¯
- ç›‘æ§ä¸­å¿ƒï¼šå®æ—¶ç›‘æ§ç³»ç»Ÿè¿è¡ŒçŠ¶æ€å’Œæ€§èƒ½æŒ‡æ ‡

1.3 æŠ€æœ¯é€‰å‹
åœ¨æŠ€æœ¯é€‰å‹æ–¹é¢ï¼Œæˆ‘ä»¬é€‰æ‹©äº†æˆç†Ÿç¨³å®šçš„æŠ€æœ¯æ ˆï¼š
- åç«¯æ¡†æ¶ï¼šSpring Boot 2.7ï¼Œæä¾›å¿«é€Ÿå¼€å‘èƒ½åŠ›
- æ•°æ®åº“ï¼šMySQL 8.0ä½œä¸ºä¸»æ•°æ®åº“ï¼ŒRedis 6.2ä½œä¸ºç¼“å­˜
- æ¶ˆæ¯é˜Ÿåˆ—ï¼šRabbitMQ 3.9ï¼Œæ”¯æŒå¼‚æ­¥å¤„ç†å’Œè§£è€¦
- å®¹å™¨åŒ–ï¼šDocker + Kubernetesï¼Œæ”¯æŒå¼¹æ€§ä¼¸ç¼©
- ç›‘æ§ï¼šPrometheus + Grafanaï¼Œæä¾›å…¨é¢çš„ç›‘æ§èƒ½åŠ›

ç¬¬äºŒç«  ç³»ç»Ÿå®ç°

æœ¬ç« è¯¦ç»†ä»‹ç»ç³»ç»Ÿçš„å…·ä½“å®ç°æ–¹æ¡ˆå’Œå…³é”®æŠ€æœ¯ç‚¹ã€‚

2.1 æœåŠ¡æ‹†åˆ†ç­–ç•¥
æœåŠ¡æ‹†åˆ†éµå¾ªå•ä¸€èŒè´£åŸåˆ™ï¼Œæ¯ä¸ªæœåŠ¡åªè´Ÿè´£ä¸€ä¸ªä¸šåŠ¡é¢†åŸŸã€‚åŒæ—¶è€ƒè™‘æ•°æ®ä¸€è‡´æ€§å’Œäº‹åŠ¡è¾¹ç•Œï¼Œé¿å…åˆ†å¸ƒå¼äº‹åŠ¡çš„å¤æ‚æ€§ã€‚

2.2 æ•°æ®å­˜å‚¨è®¾è®¡
é‡‡ç”¨è¯»å†™åˆ†ç¦»æ¶æ„ï¼Œä¸»åº“è´Ÿè´£å†™æ“ä½œï¼Œä»åº“è´Ÿè´£è¯»æ“ä½œã€‚å¯¹äºé«˜é¢‘è®¿é—®çš„æ•°æ®ï¼Œä½¿ç”¨Redisè¿›è¡Œç¼“å­˜ï¼Œæå‡ç³»ç»Ÿå“åº”é€Ÿåº¦ã€‚

2.3 å®‰å…¨æœºåˆ¶
å®ç°å¤šå±‚æ¬¡çš„å®‰å…¨é˜²æŠ¤ï¼š
- ç½‘ç»œå±‚ï¼šä½¿ç”¨é˜²ç«å¢™å’ŒVPN
- åº”ç”¨å±‚ï¼šJWTä»¤ç‰Œè®¤è¯å’ŒRBACæƒé™æ§åˆ¶
- æ•°æ®å±‚ï¼šæ•°æ®åŠ å¯†å’Œè„±æ•å¤„ç†
"""
        
        # é‡å¤æ–‡æœ¬ç›´åˆ°è¾¾åˆ°ç›®æ ‡å¤§å°
        repeated_text = base_text * (size // len(base_text) + 1)
        return repeated_text[:size]
    
    def benchmark_preset(self, preset_name: str, text_sizes: List[int], 
                        iterations: int = 3) -> Dict[str, Any]:
        """
        å¯¹æŒ‡å®šé¢„è®¾è¿›è¡ŒåŸºå‡†æµ‹è¯•
        
        Args:
            preset_name: é¢„è®¾åç§°
            text_sizes: æµ‹è¯•æ–‡æœ¬å¤§å°åˆ—è¡¨
            iterations: æ¯ä¸ªå¤§å°çš„æµ‹è¯•è¿­ä»£æ¬¡æ•°
            
        Returns:
            dict: åŸºå‡†æµ‹è¯•ç»“æœ
        """
        if not self.engine:
            raise RuntimeError("åˆ†å—å¼•æ“ä¸å¯ç”¨")
        
        results = {
            'preset_name': preset_name,
            'iterations': iterations,
            'test_results': []
        }
        
        for size in text_sizes:
            print(f"\næµ‹è¯•æ–‡æœ¬å¤§å°: {size:,} å­—ç¬¦")
            
            # ç”Ÿæˆæµ‹è¯•æ–‡æœ¬
            test_text = self.generate_test_text(size)
            metadata = {
                'file_name': f'benchmark_{size}.txt',
                'document_type': 'benchmark',
                'title': f'åŸºå‡†æµ‹è¯•æ–‡æ¡£ ({size}å­—ç¬¦)'
            }
            
            # å¤šæ¬¡æµ‹è¯•å–å¹³å‡å€¼
            times = []
            chunk_counts = []
            
            for i in range(iterations):
                print(f"  è¿­ä»£ {i+1}/{iterations}...", end=' ')
                
                start_time = time.time()
                try:
                    chunks = self.engine.chunk_document(test_text, metadata, preset_name)
                    end_time = time.time()
                    
                    processing_time = end_time - start_time
                    chunk_count = len(chunks)
                    
                    times.append(processing_time)
                    chunk_counts.append(chunk_count)
                    
                    print(f"{processing_time:.3f}s ({chunk_count}å—)")
                    
                except Exception as e:
                    print(f"å¤±è´¥: {e}")
                    continue
            
            if times:
                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                avg_time = statistics.mean(times)
                min_time = min(times)
                max_time = max(times)
                std_time = statistics.stdev(times) if len(times) > 1 else 0
                
                avg_chunks = statistics.mean(chunk_counts)
                speed = size / avg_time
                
                result = {
                    'text_size': size,
                    'avg_time': avg_time,
                    'min_time': min_time,
                    'max_time': max_time,
                    'std_time': std_time,
                    'avg_chunks': avg_chunks,
                    'speed': speed,
                    'iterations_completed': len(times)
                }
                
                results['test_results'].append(result)
                
                print(f"  å¹³å‡æ—¶é—´: {avg_time:.3f}s Â± {std_time:.3f}s")
                print(f"  å¤„ç†é€Ÿåº¦: {speed:.0f} å­—ç¬¦/ç§’")
                print(f"  å¹³å‡åˆ†å—æ•°: {avg_chunks:.1f}")
            else:
                print(f"  âŒ æ‰€æœ‰è¿­ä»£éƒ½å¤±è´¥")
        
        return results
    
    def compare_presets(self, presets: List[str], text_sizes: List[int], 
                       iterations: int = 3) -> Dict[str, Any]:
        """
        æ¯”è¾ƒå¤šä¸ªé¢„è®¾çš„æ€§èƒ½
        
        Args:
            presets: é¢„è®¾åç§°åˆ—è¡¨
            text_sizes: æµ‹è¯•æ–‡æœ¬å¤§å°åˆ—è¡¨
            iterations: æ¯ä¸ªæµ‹è¯•çš„è¿­ä»£æ¬¡æ•°
            
        Returns:
            dict: æ¯”è¾ƒç»“æœ
        """
        comparison_results = {
            'presets': presets,
            'text_sizes': text_sizes,
            'iterations': iterations,
            'results': {}
        }
        
        for preset in presets:
            print(f"\n{'='*60}")
            print(f"æµ‹è¯•é¢„è®¾: {preset}")
            print(f"{'='*60}")
            
            try:
                result = self.benchmark_preset(preset, text_sizes, iterations)
                comparison_results['results'][preset] = result
            except Exception as e:
                print(f"âŒ é¢„è®¾ {preset} æµ‹è¯•å¤±è´¥: {e}")
                comparison_results['results'][preset] = {'error': str(e)}
        
        return comparison_results
    
    def print_summary(self, results: Dict[str, Any]) -> None:
        """æ‰“å°æµ‹è¯•ç»“æœæ‘˜è¦"""
        print(f"\n{'='*80}")
        print("ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•æ‘˜è¦")
        print(f"{'='*80}")
        
        if 'results' in results:
            # å¤šé¢„è®¾æ¯”è¾ƒç»“æœ
            print(f"{'é¢„è®¾':>15} {'æ–‡æœ¬å¤§å°':>10} {'å¹³å‡æ—¶é—´':>10} {'é€Ÿåº¦':>12} {'åˆ†å—æ•°':>8}")
            print("-" * 70)
            
            for preset_name, preset_result in results['results'].items():
                if 'error' in preset_result:
                    print(f"{preset_name:>15} {'ERROR':>10} {'ERROR':>10} {'ERROR':>12} {'ERROR':>8}")
                    continue
                
                for test_result in preset_result.get('test_results', []):
                    size = test_result['text_size']
                    avg_time = test_result['avg_time']
                    speed = test_result['speed']
                    avg_chunks = test_result['avg_chunks']
                    
                    print(f"{preset_name:>15} {size:>10,} {avg_time:>9.3f}s {speed:>10.0f}/s {avg_chunks:>7.1f}")
        else:
            # å•é¢„è®¾ç»“æœ
            preset_name = results.get('preset_name', 'unknown')
            print(f"é¢„è®¾: {preset_name}")
            print(f"{'æ–‡æœ¬å¤§å°':>10} {'å¹³å‡æ—¶é—´':>10} {'é€Ÿåº¦':>12} {'åˆ†å—æ•°':>8}")
            print("-" * 45)
            
            for test_result in results.get('test_results', []):
                size = test_result['text_size']
                avg_time = test_result['avg_time']
                speed = test_result['speed']
                avg_chunks = test_result['avg_chunks']
                
                print(f"{size:>10,} {avg_time:>9.3f}s {speed:>10.0f}/s {avg_chunks:>7.1f}")
    
    def save_results(self, results: Dict[str, Any], output_file: str) -> None:
        """ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="ç®€åŒ–åˆ†å—ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument('--preset', '-p', help='æŒ‡å®šæµ‹è¯•çš„é¢„è®¾åç§°')
    parser.add_argument('--presets', nargs='+', 
                       default=['standard', 'semantic', 'structure', 'aviation_maintenance'],
                       help='æ¯”è¾ƒå¤šä¸ªé¢„è®¾ï¼ˆé»˜è®¤: standard semantic structure aviation_maintenanceï¼‰')
    parser.add_argument('--sizes', nargs='+', type=int,
                       default=[1000, 5000, 10000, 50000, 100000],
                       help='æµ‹è¯•æ–‡æœ¬å¤§å°åˆ—è¡¨ï¼ˆé»˜è®¤: 1000 5000 10000 50000 100000ï¼‰')
    parser.add_argument('--iterations', '-i', type=int, default=3,
                       help='æ¯ä¸ªæµ‹è¯•çš„è¿­ä»£æ¬¡æ•°ï¼ˆé»˜è®¤: 3ï¼‰')
    parser.add_argument('--output', '-o', help='ä¿å­˜ç»“æœçš„æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    try:
        benchmark = ChunkingBenchmark()
        
        if args.preset:
            # æµ‹è¯•å•ä¸ªé¢„è®¾
            print(f"ğŸš€ å¼€å§‹åŸºå‡†æµ‹è¯•é¢„è®¾: {args.preset}")
            results = benchmark.benchmark_preset(args.preset, args.sizes, args.iterations)
        else:
            # æ¯”è¾ƒå¤šä¸ªé¢„è®¾
            print(f"ğŸš€ å¼€å§‹æ¯”è¾ƒé¢„è®¾: {', '.join(args.presets)}")
            results = benchmark.compare_presets(args.presets, args.sizes, args.iterations)
        
        # æ‰“å°æ‘˜è¦
        benchmark.print_summary(results)
        
        # ä¿å­˜ç»“æœ
        if args.output:
            benchmark.save_results(results, args.output)
        
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
