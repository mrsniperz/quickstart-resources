#!/usr/bin/env python3
"""
æ¨¡å—åç§°: test_chunking_presets
åŠŸèƒ½æè¿°: ç®€åŒ–åˆ†å—ç³»ç»Ÿé¢„è®¾é…ç½®æµ‹è¯•è„šæœ¬ - é€‚é…æ–°çš„é¢„è®¾é…ç½®æ¶æ„
åˆ›å»ºæ—¥æœŸ: 2024-01-15
ä½œè€…: Sniperz
ç‰ˆæœ¬: v2.0.0 (ç®€åŒ–é‡æ„ç‰ˆ)

ä½¿ç”¨è¯´æ˜:
    python test_chunking_presets.py --demo                    # è¿è¡Œæ¼”ç¤ºæ¨¡å¼
    python test_chunking_presets.py -i document.txt          # æµ‹è¯•æ–‡ä»¶
    python test_chunking_presets.py -t "æµ‹è¯•æ–‡æœ¬"             # æµ‹è¯•ç›´æ¥è¾“å…¥çš„æ–‡æœ¬
    python test_chunking_presets.py --performance             # æ€§èƒ½æµ‹è¯•æ¨¡å¼
    python test_chunking_presets.py -p semantic --chunk-size 500  # è‡ªå®šä¹‰å‚æ•°
    python test_chunking_presets.py --list-presets            # åˆ—å‡ºå¯ç”¨é¢„è®¾

æ”¯æŒçš„é…ç½®é¢„è®¾ï¼ˆåŸºäºç®€åŒ–æ¶æ„ï¼‰:
    - quick: å¿«é€Ÿåˆ†å—é…ç½®
    - standard: æ ‡å‡†åˆ†å—é…ç½®
    - semantic: è¯­ä¹‰åˆ†å—é…ç½®ï¼ˆæ›¿ä»£åŸsemantic_chunkerï¼‰
    - structure: ç»“æ„åˆ†å—é…ç½®ï¼ˆæ›¿ä»£åŸstructure_chunkerï¼‰
    - aviation_maintenance: èˆªç©ºç»´ä¿®æ–‡æ¡£é…ç½®
    - aviation_regulation: èˆªç©ºè§„ç« é…ç½®
    - aviation_standard: èˆªç©ºæ ‡å‡†é…ç½®
    - aviation_training: èˆªç©ºåŸ¹è®­é…ç½®
    - high_quality: é«˜è´¨é‡åˆ†å—é…ç½®
"""

import argparse
import json
import os
import sys
import time
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥ç®€åŒ–åçš„åˆ†å—æ¨¡å—
try:
    from core.document_processor.chunking.chunking_engine import ChunkingEngine
    from core.document_processor.chunking.recursive_chunker import RecursiveCharacterChunker
    CHUNKING_ENGINE_AVAILABLE = True
except ImportError as e:
    print(f"å¯¼å…¥ChunkingEngineå¤±è´¥: {e}")
    print("å°†ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬çš„æµ‹è¯•åŠŸèƒ½")
    CHUNKING_ENGINE_AVAILABLE = False

# å°è¯•å¯¼å…¥æ—¥å¿—ç®¡ç†å™¨
try:
    from src.utils.logger import SZ_LoggerManager
    USE_CUSTOM_LOGGER = True
except ImportError:
    try:
        from utils.logger import SZ_LoggerManager
        USE_CUSTOM_LOGGER = True
    except ImportError:
        USE_CUSTOM_LOGGER = False


class SimplifiedChunkingTester:
    """
    ç®€åŒ–åˆ†å—ç³»ç»Ÿæµ‹è¯•å™¨
    
    ä¸“é—¨ä¸ºæ–°çš„é¢„è®¾é…ç½®æ¶æ„è®¾è®¡ï¼Œæä¾›ï¼š
    - é¢„è®¾é…ç½®æµ‹è¯•
    - è‡ªåŠ¨é¢„è®¾é€‰æ‹©æµ‹è¯•
    - æ€§èƒ½åŸºå‡†æµ‹è¯•
    - é…ç½®éªŒè¯
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–æµ‹è¯•å™¨
        
        Args:
            config: åˆ†å—å¼•æ“é…ç½®å‚æ•°
        """
        self.config = config or {}
        
        # è®¾ç½®æ—¥å¿—è®°å½•å™¨
        if USE_CUSTOM_LOGGER:
            self.logger = SZ_LoggerManager.setup_logger(
                logger_name="simplified_chunking_tester",
                log_file="chunking_preset_test.log",
                level=logging.INFO
            )
        else:
            logging.basicConfig(level=logging.INFO)
            self.logger = logging.getLogger("simplified_chunking_tester")
        
        try:
            if CHUNKING_ENGINE_AVAILABLE:
                # ä½¿ç”¨ç®€åŒ–åçš„ChunkingEngine
                self.engine = ChunkingEngine(self.config)
                self.logger.info("ç®€åŒ–åˆ†å—å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
            else:
                self.engine = None
                self.logger.warning("ChunkingEngineä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨åŸºç¡€æ¨¡å¼")
                
        except Exception as e:
            self.logger.error(f"åˆ†å—æµ‹è¯•å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def test_preset_chunking(self, text: str, metadata: Dict[str, Any], 
                           preset_name: Optional[str] = None) -> Dict[str, Any]:
        """
        æ‰§è¡Œé¢„è®¾é…ç½®åˆ†å—æµ‹è¯•
        
        Args:
            text: å¾…åˆ†å—çš„æ–‡æœ¬
            metadata: æ–‡æ¡£å…ƒæ•°æ®
            preset_name: æŒ‡å®šçš„é¢„è®¾é…ç½®åç§°
            
        Returns:
            dict: æµ‹è¯•ç»“æœï¼ŒåŒ…å«åˆ†å—ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯
        """
        try:
            start_time = time.time()
            
            if self.engine:
                # ä½¿ç”¨æ–°çš„é¢„è®¾é…ç½®API
                chunks = self.engine.chunk_document(text, metadata, preset_name)
            else:
                # åŸºç¡€æ¨¡å¼ï¼šç®€å•åˆ†å—
                chunks = self._basic_chunk(text, metadata)
            
            processing_time = time.time() - start_time
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            stats = self._calculate_statistics(chunks, processing_time, len(text))
            
            # åˆ›å»ºéªŒè¯ç»“æœ
            validation = self._create_validation(chunks)
            
            return {
                'chunks': chunks,
                'statistics': stats,
                'validation': validation,
                'processing_time': processing_time,
                'preset_used': preset_name or 'auto'
            }
            
        except Exception as e:
            self.logger.error(f"é¢„è®¾åˆ†å—æµ‹è¯•å¤±è´¥: {e}")
            raise
    
    def _basic_chunk(self, text: str, metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åŸºç¡€åˆ†å—å®ç°ï¼ˆå½“å¼•æ“ä¸å¯ç”¨æ—¶ï¼‰"""
        chunk_size = self.config.get('chunk_size', 1000)
        chunks = []
        
        # ç®€å•æŒ‰å¤§å°åˆ†å—
        for i in range(0, len(text), chunk_size):
            chunk_text = text[i:i + chunk_size]
            if chunk_text.strip():
                chunks.append({
                    'content': chunk_text,
                    'character_count': len(chunk_text),
                    'word_count': len(chunk_text.split()),
                    'quality_score': 0.8,
                    'metadata': {
                        'chunk_id': f"basic_{i//chunk_size:04d}",
                        'chunk_type': 'paragraph',
                        'source_document': metadata.get('file_name', 'unknown')
                    }
                })
        
        return chunks
    
    def _calculate_statistics(self, chunks: List, processing_time: float, 
                            original_length: int) -> Dict[str, Any]:
        """è®¡ç®—åˆ†å—ç»Ÿè®¡ä¿¡æ¯"""
        if not chunks:
            return {
                'chunk_count': 0,
                'total_characters': 0,
                'average_chunk_size': 0,
                'min_chunk_size': 0,
                'max_chunk_size': 0,
                'processing_speed': 0,
                'coverage_rate': 0
            }
        
        chunk_sizes = []
        total_chars = 0
        
        for chunk in chunks:
            if isinstance(chunk, dict):
                char_count = chunk.get('character_count', 0)
            else:
                char_count = getattr(chunk, 'character_count', 0)
            
            chunk_sizes.append(char_count)
            total_chars += char_count
        
        return {
            'chunk_count': len(chunks),
            'total_characters': total_chars,
            'average_chunk_size': total_chars / len(chunks),
            'min_chunk_size': min(chunk_sizes) if chunk_sizes else 0,
            'max_chunk_size': max(chunk_sizes) if chunk_sizes else 0,
            'processing_speed': original_length / processing_time if processing_time > 0 else 0,
            'coverage_rate': (total_chars / original_length) * 100 if original_length > 0 else 0
        }
    
    def _create_validation(self, chunks: List) -> Dict[str, Any]:
        """åˆ›å»ºéªŒè¯ç»“æœ"""
        if not chunks:
            return {
                'total_chunks': 0,
                'valid_chunks': 0,
                'invalid_chunks': 0,
                'avg_quality_score': 0.0,
                'issues': []
            }
        
        total_chunks = len(chunks)
        quality_scores = []
        
        for chunk in chunks:
            if isinstance(chunk, dict):
                quality = chunk.get('quality_score')
            else:
                quality = getattr(chunk, 'quality_score', None)

            # åªæœ‰å½“è´¨é‡è¯„åˆ†ä¸ä¸ºNoneæ—¶æ‰æ·»åŠ åˆ°åˆ—è¡¨ä¸­
            if quality is not None:
                quality_scores.append(quality)

        # å¦‚æœæœ‰è´¨é‡è¯„åˆ†ï¼Œè®¡ç®—å¹³å‡å€¼ï¼›å¦åˆ™è¿”å›Noneè¡¨ç¤ºæœªè¯„ä¼°
        avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else None
        
        return {
            'total_chunks': total_chunks,
            'valid_chunks': total_chunks,
            'invalid_chunks': 0,
            'avg_quality_score': avg_quality,
            'issues': []
        }
    
    def list_available_presets(self) -> None:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„é¢„è®¾é…ç½®"""
        print("\n" + "="*80)
        print("ğŸ“‹ å¯ç”¨çš„é¢„è®¾é…ç½®ï¼ˆç®€åŒ–æ¶æ„ï¼‰")
        print("="*80)
        
        if self.engine:
            try:
                # è·å–é¢„è®¾é…ç½®
                config_manager = None
                try:
                    from core.document_processor.config.config_manager import get_config_manager
                    config_manager = get_config_manager()
                except ImportError:
                    try:
                        from src.core.document_processor.config.config_manager import get_config_manager
                        config_manager = get_config_manager()
                    except ImportError:
                        print("âŒ é…ç½®ç®¡ç†å™¨å¯¼å…¥å¤±è´¥ï¼Œæ— æ³•è·å–é¢„è®¾ä¿¡æ¯")
                
                if config_manager:
                    # ç›´æ¥ä»é…ç½®æ–‡ä»¶è·å–é¢„è®¾é…ç½®
                    chunking_config = config_manager.get_chunking_config()
                    presets = chunking_config.get('presets', {})
                    
                    if not presets:
                        print("âŒ æ²¡æœ‰å¯ç”¨çš„é¢„è®¾é…ç½®")
                        return
                    
                    for preset_name, preset_config in presets.items():
                        print(f"\nğŸ”¸ {preset_name}")
                        print(f"   æè¿°: {preset_config.get('description', 'æ— æè¿°')}")
                        print(f"   åˆ†å—å¤§å°: {preset_config.get('chunk_size', 'æœªçŸ¥')}")
                        print(f"   é‡å å¤§å°: {preset_config.get('chunk_overlap', 'æœªçŸ¥')}")
                        print(f"   åˆ†éš”ç¬¦æ•°é‡: {len(preset_config.get('separators', []))}")
                else:
                    # ä½¿ç”¨å¼•æ“çš„APIè·å–é¢„è®¾
                    presets = self.engine.get_available_presets()
                    
                    if not presets:
                        print("âŒ æ²¡æœ‰å¯ç”¨çš„é¢„è®¾é…ç½®")
                        return
                    
                    for preset in presets:
                        # è·³è¿‡éé¢„è®¾é…ç½®é¡¹
                        if preset in ['default_strategy', 'chunk_size', 'chunk_overlap', 
                                    'min_chunk_size', 'max_chunk_size', 'preserve_context',
                                    'enable_quality_assessment', 'quality_strategy']:
                            continue
                        
                        try:
                            info = self.engine.get_preset_info(preset)
                            print(f"\nğŸ”¸ {preset}")
                            print(f"   æè¿°: {info.get('description', 'æ— æè¿°')}")
                            print(f"   åˆ†å—å¤§å°: {info.get('chunk_size', 'æœªçŸ¥')}")
                            print(f"   é‡å å¤§å°: {info.get('chunk_overlap', 'æœªçŸ¥')}")
                            print(f"   åˆ†éš”ç¬¦æ•°é‡: {info.get('separators_count', 'æœªçŸ¥')}")
                            
                            if 'error' in info:
                                print(f"   âŒ é”™è¯¯: {info['error']}")
                        except Exception as e:
                            print(f"è·å–é¢„è®¾ä¿¡æ¯å¤±è´¥: {e}")
                        
            except Exception as e:
                print(f"âŒ è·å–é¢„è®¾ä¿¡æ¯å¤±è´¥: {e}")
        else:
            print("âŒ åˆ†å—å¼•æ“ä¸å¯ç”¨ï¼Œæ— æ³•è·å–é¢„è®¾ä¿¡æ¯")
            print("ğŸ“Œ åŸºç¡€æ¨¡å¼æ”¯æŒçš„é¢„è®¾: basic")
    
    def compare_presets(self, text: str, metadata: Dict[str, Any]) -> None:
        """
        æ¯”è¾ƒä¸åŒé¢„è®¾çš„åˆ†å—æ•ˆæœ
        
        Args:
            text: å¾…åˆ†å—çš„æ–‡æœ¬
            metadata: æ–‡æ¡£å…ƒæ•°æ®
        """
        print("\n" + "="*80)
        print("ğŸ” é¢„è®¾é…ç½®å¯¹æ¯”åˆ†æ")
        print("="*80)
        
        if not self.engine:
            print("âŒ åˆ†å—å¼•æ“ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œé¢„è®¾å¯¹æ¯”")
            return
        
        presets = self.engine.get_available_presets()
        results = {}
        
        # æµ‹è¯•ä¸»è¦é¢„è®¾
        test_presets = ['standard', 'semantic', 'structure', 'aviation_maintenance', 'high_quality']
        test_presets = [p for p in test_presets if p in presets]
        
        for preset in test_presets:
            print(f"\næµ‹è¯•é¢„è®¾: {preset}")
            try:
                result = self.test_preset_chunking(text, metadata, preset)
                results[preset] = result
                
                stats = result['statistics']
                print(f"  åˆ†å—æ•°é‡: {stats['chunk_count']}")
                print(f"  å¤„ç†æ—¶é—´: {result['processing_time']:.3f}ç§’")
                print(f"  å¹³å‡å¤§å°: {stats['average_chunk_size']:.1f}å­—ç¬¦")
                quality_score = result['validation'].get('avg_quality_score')
                if quality_score is not None:
                    print(f"  è´¨é‡è¯„åˆ†: {quality_score:.3f}")
                else:
                    print(f"  è´¨é‡è¯„åˆ†: æœªè¯„ä¼°")
                
            except Exception as e:
                print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
                self.logger.error(f"é¢„è®¾ {preset} æµ‹è¯•å¤±è´¥: {e}")
        
        # è¾“å‡ºå¯¹æ¯”æ€»ç»“
        if len(results) > 1:
            print(f"\nğŸ“Š å¯¹æ¯”æ€»ç»“:")
            print(f"{'é¢„è®¾':>15} {'åˆ†å—æ•°':>8} {'æ—¶é—´(s)':>10} {'å¹³å‡å¤§å°':>10} {'è´¨é‡':>8}")
            print("-" * 60)
            
            for preset, result in results.items():
                stats = result['statistics']
                quality = result['validation'].get('avg_quality_score', 0)
                print(f"{preset:>15} {stats['chunk_count']:>8} "
                      f"{result['processing_time']:>9.3f} {stats['average_chunk_size']:>9.1f} "
                      f"{quality:>7.3f}")

    def test_automatic_preset_selection(self, test_cases: List[Dict[str, Any]]) -> None:
        """
        æµ‹è¯•è‡ªåŠ¨é¢„è®¾é€‰æ‹©åŠŸèƒ½

        Args:
            test_cases: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
        """
        print("\n" + "="*80)
        print("ğŸ¤– è‡ªåŠ¨é¢„è®¾é€‰æ‹©æµ‹è¯•")
        print("="*80)

        if not self.engine:
            print("âŒ åˆ†å—å¼•æ“ä¸å¯ç”¨ï¼Œæ— æ³•æµ‹è¯•è‡ªåŠ¨é¢„è®¾é€‰æ‹©")
            return

        for i, case in enumerate(test_cases, 1):
            print(f"\n--- æµ‹è¯•ç”¨ä¾‹ {i} ---")
            print(f"æ–‡æ¡£å…ƒæ•°æ®: {case['metadata']}")
            print(f"æœŸæœ›é¢„è®¾: {case.get('expected_preset', 'æœªçŸ¥')}")

            try:
                # ä¸æŒ‡å®šé¢„è®¾ï¼Œè®©å¼•æ“è‡ªåŠ¨é€‰æ‹©
                result = self.test_preset_chunking(
                    case['text'],
                    case['metadata']
                )

                print(f"å®é™…ä½¿ç”¨é¢„è®¾: {result['preset_used']}")
                print(f"åˆ†å—æ•°é‡: {result['statistics']['chunk_count']}")

                # æ£€æŸ¥æ˜¯å¦ç¬¦åˆé¢„æœŸ
                expected = case.get('expected_preset')
                if expected and result['preset_used'] == expected:
                    print("âœ… é¢„è®¾é€‰æ‹©æ­£ç¡®")
                elif expected:
                    print(f"âš ï¸  é¢„è®¾é€‰æ‹©ä¸ç¬¦åˆé¢„æœŸï¼ŒæœŸæœ›: {expected}")
                else:
                    print("â„¹ï¸  æ— é¢„æœŸé¢„è®¾ï¼Œä»…éªŒè¯åŠŸèƒ½")

            except Exception as e:
                print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")

    def test_quality_assessment(self, text: str, metadata: Dict[str, Any]) -> None:
        """
        æµ‹è¯•è´¨é‡æ£€æµ‹åŠŸèƒ½

        Args:
            text: å¾…æµ‹è¯•çš„æ–‡æœ¬
            metadata: æ–‡æ¡£å…ƒæ•°æ®
        """
        print("\n" + "="*80)
        print("ğŸ” è´¨é‡æ£€æµ‹åŠŸèƒ½æµ‹è¯•")
        print("="*80)

        if not self.engine:
            print("âŒ åˆ†å—å¼•æ“ä¸å¯ç”¨ï¼Œæ— æ³•æµ‹è¯•è´¨é‡æ£€æµ‹åŠŸèƒ½")
            return

        # æµ‹è¯•ä¸åŒè´¨é‡æ£€æµ‹ç­–ç•¥
        quality_strategies = ['basic', 'strict', 'disabled']
        results = {}

        for strategy in quality_strategies:
            print(f"\n--- æµ‹è¯•è´¨é‡æ£€æµ‹ç­–ç•¥: {strategy} ---")

            try:
                # ä¸´æ—¶è®¾ç½®è´¨é‡æ£€æµ‹ç­–ç•¥
                if hasattr(self.engine, 'set_quality_assessment_strategy'):
                    success = self.engine.set_quality_assessment_strategy(strategy)
                    if not success:
                        print(f"âš ï¸  è®¾ç½®è´¨é‡æ£€æµ‹ç­–ç•¥å¤±è´¥: {strategy}")
                        continue

                # æ‰§è¡Œåˆ†å—æµ‹è¯•
                start_time = time.time()
                result = self.test_preset_chunking(text, metadata, 'standard')
                processing_time = time.time() - start_time

                results[strategy] = result

                # è¾“å‡ºæµ‹è¯•ç»“æœ
                stats = result['statistics']
                validation = result['validation']

                print(f"  åˆ†å—æ•°é‡: {stats['chunk_count']}")
                print(f"  å¤„ç†æ—¶é—´: {processing_time:.3f}ç§’")
                quality_score = validation.get('avg_quality_score')
                if quality_score is not None:
                    print(f"  å¹³å‡è´¨é‡è¯„åˆ†: {quality_score:.3f}")
                else:
                    print(f"  å¹³å‡è´¨é‡è¯„åˆ†: æœªè¯„ä¼°")
                print(f"  å¹³å‡åˆ†å—å¤§å°: {stats['average_chunk_size']:.1f}å­—ç¬¦")

                # åˆ†æè´¨é‡æ£€æµ‹æ•ˆæœ
                if strategy == 'disabled':
                    print("  ğŸ“ è´¨é‡æ£€æµ‹å·²ç¦ç”¨ï¼Œæ‰€æœ‰åˆ†å—è´¨é‡è¯„åˆ†ä¸ºé»˜è®¤å€¼")
                elif strategy == 'basic':
                    print("  ğŸ“ ä½¿ç”¨åŸºç¡€è´¨é‡æ£€æµ‹ï¼Œè¯„ä¼°é•¿åº¦å’Œå®Œæ•´æ€§")
                elif strategy == 'strict':
                    print("  ğŸ“ ä½¿ç”¨ä¸¥æ ¼è´¨é‡æ£€æµ‹ï¼Œæ›´é«˜çš„è´¨é‡æ ‡å‡†")

            except Exception as e:
                print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
                self.logger.error(f"è´¨é‡æ£€æµ‹ç­–ç•¥ {strategy} æµ‹è¯•å¤±è´¥: {e}")

        # è¾“å‡ºå¯¹æ¯”æ€»ç»“
        if len(results) > 1:
            print(f"\nğŸ“Š è´¨é‡æ£€æµ‹ç­–ç•¥å¯¹æ¯”:")
            print(f"{'ç­–ç•¥':>10} {'åˆ†å—æ•°':>8} {'å¹³å‡è´¨é‡':>10} {'å¤„ç†æ—¶é—´':>10}")
            print("-" * 45)

            for strategy, result in results.items():
                stats = result['statistics']
                validation = result['validation']
                quality_score = validation.get('avg_quality_score')
                quality_str = f"{quality_score:>9.3f}" if quality_score is not None else "    æœªè¯„ä¼°"
                print(f"{strategy:>10} {stats['chunk_count']:>8} "
                      f"{quality_str} {result['processing_time']:>9.3f}s")

        # è¯¦ç»†è´¨é‡åˆ†æ
        self._analyze_quality_impact(results)

    def _analyze_quality_impact(self, results: Dict[str, Dict[str, Any]]) -> None:
        """
        åˆ†æè´¨é‡æ£€æµ‹å¯¹åˆ†å—ç»“æœçš„å½±å“

        Args:
            results: ä¸åŒç­–ç•¥çš„æµ‹è¯•ç»“æœ
        """
        if len(results) < 2:
            return

        print(f"\nğŸ”¬ è´¨é‡æ£€æµ‹å½±å“åˆ†æ:")

        # è·å–åŸºå‡†ç»“æœï¼ˆdisabledç­–ç•¥ï¼‰
        baseline = results.get('disabled')
        if not baseline:
            baseline = list(results.values())[0]

        baseline_stats = baseline['statistics']
        baseline_time = baseline['processing_time']

        for strategy, result in results.items():
            if strategy == 'disabled':
                continue

            stats = result['statistics']
            validation = result['validation']

            # è®¡ç®—æ€§èƒ½å½±å“
            time_overhead = ((result['processing_time'] - baseline_time) / baseline_time) * 100
            chunk_diff = stats['chunk_count'] - baseline_stats['chunk_count']
            quality_score = validation.get('avg_quality_score', 0)

            print(f"\n  ğŸ“ˆ {strategy} ç­–ç•¥å½±å“:")
            print(f"     æ—¶é—´å¼€é”€: {time_overhead:+.1f}%")
            print(f"     åˆ†å—æ•°é‡å˜åŒ–: {chunk_diff:+d}")
            if quality_score is not None:
                print(f"     è´¨é‡è¯„åˆ†: {quality_score:.3f}")
            else:
                print(f"     è´¨é‡è¯„åˆ†: æœªè¯„ä¼°")

            # ç»™å‡ºå»ºè®®
            if time_overhead < 5:
                print(f"     ğŸ’¡ å»ºè®®: æ€§èƒ½å½±å“å¾ˆå°ï¼Œæ¨èä½¿ç”¨")
            elif time_overhead < 20:
                print(f"     ğŸ’¡ å»ºè®®: æ€§èƒ½å½±å“é€‚ä¸­ï¼Œå¯æ ¹æ®éœ€è¦ä½¿ç”¨")
            else:
                print(f"     ğŸ’¡ å»ºè®®: æ€§èƒ½å½±å“è¾ƒå¤§ï¼Œä»…åœ¨è´¨é‡è¦æ±‚é«˜æ—¶ä½¿ç”¨")

    def visualize_chunks(self, result: Dict[str, Any], output_format: str = 'detailed') -> None:
        """
        å¯è§†åŒ–åˆ†å—ç»“æœ

        Args:
            result: æµ‹è¯•ç»“æœ
            output_format: è¾“å‡ºæ ¼å¼ ('detailed', 'simple', 'json')
        """
        chunks = result['chunks']
        stats = result['statistics']
        validation = result['validation']

        if output_format == 'json':
            self._output_json(result)
            return

        # è¾“å‡ºæ ‡é¢˜
        print("\n" + "="*80)
        print(f"ğŸ” ç®€åŒ–åˆ†å—ç³»ç»Ÿæµ‹è¯•ç»“æœ")
        print(f"ğŸ“Š é¢„è®¾: {result['preset_used']}")
        print(f"â±ï¸  å¤„ç†æ—¶é—´: {result['processing_time']:.3f}ç§’")
        print("="*80)

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        self._print_statistics(stats, validation)

        if output_format == 'detailed':
            self._print_detailed_chunks(chunks)
        else:
            self._print_simple_chunks(chunks)

    def _print_statistics(self, stats: Dict[str, Any], validation: Dict[str, Any]) -> None:
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   åˆ†å—æ•°é‡: {stats['chunk_count']}")
        print(f"   æ€»å­—ç¬¦æ•°: {stats['total_characters']}")
        print(f"   å¹³å‡åˆ†å—å¤§å°: {stats['average_chunk_size']:.1f} å­—ç¬¦")
        print(f"   æœ€å°åˆ†å—: {stats['min_chunk_size']} å­—ç¬¦")
        print(f"   æœ€å¤§åˆ†å—: {stats['max_chunk_size']} å­—ç¬¦")
        print(f"   å¤„ç†é€Ÿåº¦: {stats['processing_speed']:.0f} å­—ç¬¦/ç§’")
        print(f"   è¦†ç›–ç‡: {stats['coverage_rate']:.1f}%")

        # éªŒè¯ä¿¡æ¯
        print(f"\nğŸ” è´¨é‡éªŒè¯:")
        print(f"   æœ‰æ•ˆåˆ†å—: {validation['valid_chunks']}")
        print(f"   æ— æ•ˆåˆ†å—: {validation['invalid_chunks']}")
        quality_score = validation.get('avg_quality_score')
        if quality_score is not None:
            print(f"   å¹³å‡è´¨é‡è¯„åˆ†: {quality_score:.3f}")
        else:
            print(f"   å¹³å‡è´¨é‡è¯„åˆ†: æœªè¯„ä¼°")

        if validation.get('issues'):
            print(f"   âš ï¸  å‘ç°é—®é¢˜: {len(validation['issues'])}ä¸ª")
            for issue in validation['issues'][:3]:
                print(f"      - {issue}")
            if len(validation['issues']) > 3:
                print(f"      - ... è¿˜æœ‰{len(validation['issues']) - 3}ä¸ªé—®é¢˜")

    def _print_detailed_chunks(self, chunks: List) -> None:
        """æ‰“å°è¯¦ç»†åˆ†å—ä¿¡æ¯"""
        print(f"\nğŸ“ è¯¦ç»†åˆ†å—ç»“æœ:")

        for i, chunk in enumerate(chunks, 1):
            print(f"\n--- åˆ†å— {i} ---")

            # å¤„ç†ä¸åŒæ ¼å¼çš„chunk
            if isinstance(chunk, dict):
                char_count = chunk.get('character_count', 0)
                word_count = chunk.get('word_count', 0)
                quality_score = chunk.get('quality_score', 0)
                content = chunk.get('content', '')
                metadata = chunk.get('metadata', {})
            else:
                char_count = getattr(chunk, 'character_count', 0)
                word_count = getattr(chunk, 'word_count', 0)
                quality_score = getattr(chunk, 'quality_score', 0)
                content = getattr(chunk, 'content', '')
                metadata = getattr(chunk, 'metadata', {})

            print(f"å¤§å°: {char_count} å­—ç¬¦ | è¯æ•°: {word_count}")
            if quality_score is not None:
                print(f"è´¨é‡è¯„åˆ†: {quality_score:.3f}")
            else:
                print(f"è´¨é‡è¯„åˆ†: æœªè¯„ä¼°")

            # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
            content_preview = content[:200] + "..." if len(content) > 200 else content
            print(f"å†…å®¹: {content_preview}")

    def _print_simple_chunks(self, chunks: List) -> None:
        """æ‰“å°ç®€æ´åˆ†å—ä¿¡æ¯"""
        print(f"\nğŸ“‹ åˆ†å—æ¦‚è§ˆ:")

        for i, chunk in enumerate(chunks, 1):
            # å¤„ç†ä¸åŒæ ¼å¼çš„chunk
            if isinstance(chunk, dict):
                char_count = chunk.get('character_count', 0)
                quality_score = chunk.get('quality_score', 0)
                content = chunk.get('content', '')
            else:
                char_count = getattr(chunk, 'character_count', 0)
                quality_score = getattr(chunk, 'quality_score', 0)
                content = getattr(chunk, 'content', '')

            content_preview = content[:50] + "..." if len(content) > 50 else content
            quality_info = f" (è´¨é‡: {quality_score:.2f})" if quality_score > 0 else ""
            print(f"  {i:2d}. [{char_count:4d}å­—ç¬¦] {content_preview}{quality_info}")

    def _output_json(self, result: Dict[str, Any]) -> None:
        """è¾“å‡ºå¢å¼ºçš„JSONæ ¼å¼ç»“æœï¼ŒåŒ…å«è¯¦ç»†çš„è¯„åˆ†æ ‡å‡†å’Œæ£€æµ‹é€»è¾‘ä¿¡æ¯"""
        # è½¬æ¢chunksä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        serializable_chunks = []
        for chunk in result['chunks']:
            if isinstance(chunk, dict):
                chunk_data = chunk.copy()
            else:
                chunk_data = {
                    'content': getattr(chunk, 'content', ''),
                    'character_count': getattr(chunk, 'character_count', 0),
                    'word_count': getattr(chunk, 'word_count', 0),
                    'quality_score': getattr(chunk, 'quality_score', None),
                    'metadata': {}
                }

                # å¤„ç†metadata
                metadata = getattr(chunk, 'metadata', None)
                if metadata:
                    if isinstance(metadata, dict):
                        chunk_data['metadata'] = metadata
                    else:
                        chunk_data['metadata'] = {
                            'chunk_id': getattr(metadata, 'chunk_id', ''),
                            'chunk_type': str(getattr(metadata, 'chunk_type', '')),
                            'source_document': getattr(metadata, 'source_document', '')
                        }

            serializable_chunks.append(chunk_data)

        # è·å–æœ¬æ¬¡æµ‹è¯•çš„å®é™…é…ç½®ä¿¡æ¯
        test_metadata = self._get_test_specific_metadata(result)

        output = {
            'preset_used': result['preset_used'],
            'processing_time': result['processing_time'],
            'statistics': result['statistics'],
            'validation': result['validation'],
            'chunks': serializable_chunks,
            # ç®€åŒ–çš„å…ƒæ•°æ®ï¼šåªåŒ…å«æœ¬æ¬¡æµ‹è¯•çš„å®é™…ä¿¡æ¯
            'metadata': test_metadata
        }

        print(json.dumps(output, ensure_ascii=False, indent=2))

    def _get_test_specific_metadata(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """è·å–æœ¬æ¬¡æµ‹è¯•çš„å®é™…é…ç½®å’Œç­–ç•¥ä¿¡æ¯"""
        try:
            metadata = {}

            # 1. è·å–å®é™…ä½¿ç”¨çš„è´¨é‡è¯„ä¼°ç­–ç•¥ä¿¡æ¯
            if self.engine and hasattr(self.engine, 'get_quality_assessment_info'):
                quality_info = self.engine.get_quality_assessment_info()
                if 'error' not in quality_info:
                    current_strategy = quality_info.get('current_strategy', 'unknown')
                    strategy_info = quality_info.get('strategy_info', {})

                    metadata['quality_assessment'] = {
                        'strategy_name': current_strategy,
                        'enabled': quality_info.get('enabled', False),
                        'config': strategy_info.get('config', {}),
                        'preset': strategy_info.get('preset', 'unknown')
                    }

                    # åªæœ‰åœ¨å¯ç”¨æ—¶æ‰æ·»åŠ è¯„åˆ†è®¡ç®—æ–¹å¼
                    if quality_info.get('enabled', False) and current_strategy != 'disabled':
                        config = strategy_info.get('config', {})
                        if current_strategy == 'basic':
                            metadata['quality_assessment']['score_calculation'] = {
                                'method': 'weighted_average',
                                'length_weight': config.get('length_weight', 0.6),
                                'completeness_weight': config.get('completeness_weight', 0.4),
                                'formula': f"length_score * {config.get('length_weight', 0.6)} + completeness_score * {config.get('completeness_weight', 0.4)}"
                            }
                        elif current_strategy == 'strict':
                            metadata['quality_assessment']['score_calculation'] = {
                                'method': 'weighted_average',
                                'length_weight': config.get('length_weight', 0.5),
                                'completeness_weight': config.get('completeness_weight', 0.5),
                                'formula': f"length_score * {config.get('length_weight', 0.5)} + completeness_score * {config.get('completeness_weight', 0.5)}"
                            }
                else:
                    metadata['quality_assessment'] = {'error': quality_info.get('error')}
            else:
                metadata['quality_assessment'] = {'status': 'unavailable'}

            # 2. è·å–æœ¬æ¬¡æµ‹è¯•çš„åˆ†å—é…ç½®
            metadata['chunking_config'] = {
                'chunk_size': self.config.get('chunk_size'),
                'chunk_overlap': self.config.get('chunk_overlap'),
                'min_chunk_size': self.config.get('min_chunk_size'),
                'max_chunk_size': self.config.get('max_chunk_size'),
                'preserve_context': self.config.get('preserve_context'),
                'enable_quality_assessment': self.config.get('enable_quality_assessment'),
                'quality_strategy': self.config.get('quality_strategy')
            }

            # 3. è·å–validationçš„å®é™…ç»“æœè¯´æ˜
            validation = result.get('validation', {})
            avg_score = validation.get('avg_quality_score')
            metadata['validation_info'] = {
                'method': 'average_non_null_scores',
                'total_chunks_evaluated': validation.get('total_chunks', 0),
                'chunks_with_scores': len([1 for chunk in result.get('chunks', [])
                                         if self._get_chunk_quality_score(chunk) is not None]),
                'avg_calculation': 'sum(non_null_scores) / count(non_null_scores)' if avg_score is not None else 'no_scores_available'
            }

            return metadata

        except Exception as e:
            return {'error': f'è·å–æµ‹è¯•å…ƒæ•°æ®å¤±è´¥: {e}'}

    def _get_chunk_quality_score(self, chunk) -> Optional[float]:
        """è·å–åˆ†å—çš„è´¨é‡è¯„åˆ†"""
        if isinstance(chunk, dict):
            return chunk.get('quality_score')
        else:
            return getattr(chunk, 'quality_score', None)

    def run_performance_test(self, text_sizes: List[int] = None) -> None:
        """
        è¿è¡Œæ€§èƒ½æµ‹è¯•

        Args:
            text_sizes: æµ‹è¯•æ–‡æœ¬å¤§å°åˆ—è¡¨ï¼ˆå­—ç¬¦æ•°ï¼‰
        """
        if text_sizes is None:
            text_sizes = [1000, 5000, 10000, 50000, 100000]

        print("\n" + "="*80)
        print("ğŸš€ ç®€åŒ–åˆ†å—ç³»ç»Ÿæ€§èƒ½æµ‹è¯•")
        print("="*80)

        # ç”Ÿæˆæµ‹è¯•æ–‡æœ¬
        base_text = self._get_sample_text('performance')

        results = []

        for size in text_sizes:
            # ç”ŸæˆæŒ‡å®šå¤§å°çš„æ–‡æœ¬
            test_text = (base_text * (size // len(base_text) + 1))[:size]

            metadata = {
                'file_name': f'performance_test_{size}.txt',
                'document_type': 'performance_test',
                'title': f'æ€§èƒ½æµ‹è¯•æ–‡æ¡£ ({size}å­—ç¬¦)'
            }

            print(f"\næµ‹è¯•æ–‡æœ¬å¤§å°: {size:,} å­—ç¬¦")

            try:
                result = self.test_preset_chunking(test_text, metadata, 'standard')
                results.append({
                    'size': size,
                    'time': result['processing_time'],
                    'chunks': result['statistics']['chunk_count'],
                    'speed': result['statistics']['processing_speed']
                })

                print(f"  å¤„ç†æ—¶é—´: {result['processing_time']:.3f}ç§’")
                print(f"  åˆ†å—æ•°é‡: {result['statistics']['chunk_count']}")
                print(f"  å¤„ç†é€Ÿåº¦: {result['statistics']['processing_speed']:.0f} å­—ç¬¦/ç§’")

            except Exception as e:
                print(f"  æµ‹è¯•å¤±è´¥: {e}")

        # è¾“å‡ºæ€§èƒ½æ€»ç»“
        if results:
            print(f"\nğŸ“Š æ€§èƒ½æµ‹è¯•æ€»ç»“:")
            print(f"{'æ–‡æœ¬å¤§å°':>10} {'å¤„ç†æ—¶é—´':>10} {'åˆ†å—æ•°':>8} {'é€Ÿåº¦':>12}")
            print("-" * 45)
            for r in results:
                print(f"{r['size']:>10,} {r['time']:>9.3f}s {r['chunks']:>7} {r['speed']:>10.0f}/s")

    def run_demo(self) -> None:
        """è¿è¡Œæ¼”ç¤ºæ¨¡å¼"""
        print("\n" + "="*80)
        print("ğŸ¯ ç®€åŒ–åˆ†å—ç³»ç»ŸåŠŸèƒ½æ¼”ç¤º")
        print("="*80)

        demo_scenarios = [
            ('é€šç”¨æŠ€æœ¯æ–‡æ¡£', 'general', 'standard'),
            ('èˆªç©ºç»´ä¿®æ‰‹å†Œ', 'aviation', 'aviation_maintenance'),
            ('ç»“æ„åŒ–æ–‡æ¡£', 'structured', 'structure'),
            ('è¯­ä¹‰è¿è´¯æ–‡æ¡£', 'semantic', 'semantic')
        ]

        for name, text_type, expected_preset in demo_scenarios:
            print(f"\nğŸ”¸ æ¼”ç¤ºåœºæ™¯: {name}")
            print("-" * 40)

            text = self._get_sample_text(text_type)
            metadata = {
                'file_name': f'{text_type}_demo.txt',
                'document_type': text_type,
                'title': name
            }

            try:
                # æµ‹è¯•è‡ªåŠ¨é¢„è®¾é€‰æ‹©
                result = self.test_preset_chunking(text, metadata)
                print(f"è‡ªåŠ¨é€‰æ‹©é¢„è®¾: {result['preset_used']}")

                # æµ‹è¯•æŒ‡å®šé¢„è®¾
                if expected_preset:
                    result_preset = self.test_preset_chunking(text, metadata, expected_preset)
                    print(f"æŒ‡å®šé¢„è®¾æ•ˆæœ: {expected_preset}")

                    # ç®€å•å¯¹æ¯”
                    auto_chunks = result['statistics']['chunk_count']
                    preset_chunks = result_preset['statistics']['chunk_count']
                    print(f"  è‡ªåŠ¨é€‰æ‹©: {auto_chunks}ä¸ªåˆ†å—")
                    print(f"  æŒ‡å®šé¢„è®¾: {preset_chunks}ä¸ªåˆ†å—")

                self.visualize_chunks(result, 'simple')

            except Exception as e:
                print(f"æ¼”ç¤ºå¤±è´¥: {e}")
                self.logger.error(f"æ¼”ç¤ºåœºæ™¯ {name} å¤±è´¥: {e}")

    def _get_sample_text(self, text_type: str) -> str:
        """è·å–ç¤ºä¾‹æ–‡æœ¬"""
        samples = {
            'general': """
ç¬¬ä¸€ç«  ç³»ç»Ÿæ¶æ„è®¾è®¡

1.1 æ¦‚è¿°
æœ¬ç³»ç»Ÿé‡‡ç”¨å¾®æœåŠ¡æ¶æ„è®¾è®¡ï¼Œå…·æœ‰é«˜å¯ç”¨æ€§ã€å¯æ‰©å±•æ€§å’Œå¯ç»´æŠ¤æ€§çš„ç‰¹ç‚¹ã€‚ç³»ç»Ÿä¸»è¦ç”±ä»¥ä¸‹å‡ ä¸ªæ ¸å¿ƒæ¨¡å—ç»„æˆï¼šç”¨æˆ·ç®¡ç†æ¨¡å—ã€æ•°æ®å¤„ç†æ¨¡å—ã€æ¥å£æœåŠ¡æ¨¡å—å’Œç›‘æ§æ¨¡å—ã€‚

1.2 æŠ€æœ¯é€‰å‹
åœ¨æŠ€æœ¯é€‰å‹æ–¹é¢ï¼Œæˆ‘ä»¬é€‰æ‹©äº†ä»¥ä¸‹æŠ€æœ¯æ ˆï¼š
- åç«¯æ¡†æ¶ï¼šSpring Boot 2.7
- æ•°æ®åº“ï¼šMySQL 8.0 + Redis 6.2
- æ¶ˆæ¯é˜Ÿåˆ—ï¼šRabbitMQ 3.9
- å®¹å™¨åŒ–ï¼šDocker + Kubernetes
- ç›‘æ§ï¼šPrometheus + Grafana

1.3 ç³»ç»Ÿç‰¹æ€§
ç³»ç»Ÿå…·å¤‡ä»¥ä¸‹æ ¸å¿ƒç‰¹æ€§ï¼š
1. é«˜å¹¶å‘å¤„ç†èƒ½åŠ›ï¼Œæ”¯æŒæ¯ç§’10ä¸‡æ¬¡è¯·æ±‚
2. æ•°æ®ä¸€è‡´æ€§ä¿è¯ï¼Œé‡‡ç”¨åˆ†å¸ƒå¼äº‹åŠ¡ç®¡ç†
3. è‡ªåŠ¨æ•…éšœæ¢å¤ï¼Œå…·å¤‡å®Œå–„çš„å®¹é”™æœºåˆ¶
4. å®æ—¶ç›‘æ§å‘Šè­¦ï¼Œç¡®ä¿ç³»ç»Ÿç¨³å®šè¿è¡Œ
""",

            'aviation': """
ä»»åŠ¡1ï¼šå‘åŠ¨æœºæ—¥å¸¸æ£€æŸ¥ç¨‹åº

è­¦å‘Šï¼šåœ¨è¿›è¡Œä»»ä½•å‘åŠ¨æœºæ£€æŸ¥å‰ï¼Œå¿…é¡»ç¡®ä¿å‘åŠ¨æœºå®Œå…¨å†·å´ï¼Œå¹¶æ–­å¼€æ‰€æœ‰ç”µæºã€‚

æ­¥éª¤1ï¼šå¤–è§‚æ£€æŸ¥
æ£€æŸ¥å‘åŠ¨æœºå¤–å£³æ˜¯å¦æœ‰è£‚çº¹ã€è…èš€æˆ–å¼‚å¸¸ç£¨æŸã€‚ç‰¹åˆ«æ³¨æ„ä»¥ä¸‹éƒ¨ä½ï¼š
- è¿›æ°”é“å’Œæ’æ°”å£
- ç‡ƒæ²¹ç®¡è·¯è¿æ¥å¤„
- ç”µæ°”çº¿æŸå›ºå®šç‚¹
- å†·å´ç³»ç»Ÿç®¡è·¯

æ­¥éª¤2ï¼šæ¶²ä½æ£€æŸ¥
æ£€æŸ¥å„ç§æ¶²ä½“çš„æ¶²ä½æ˜¯å¦åœ¨æ­£å¸¸èŒƒå›´å†…ï¼š
- å‘åŠ¨æœºæœºæ²¹æ¶²ä½
- å†·å´æ¶²æ¶²ä½
- æ¶²å‹æ²¹æ¶²ä½

æ³¨æ„ï¼šæ‰€æœ‰æ¶²ä½æ£€æŸ¥å¿…é¡»åœ¨å‘åŠ¨æœºæ°´å¹³çŠ¶æ€ä¸‹è¿›è¡Œã€‚

æ­¥éª¤3ï¼šåŠŸèƒ½æµ‹è¯•
å¯åŠ¨å‘åŠ¨æœºè¿›è¡ŒåŠŸèƒ½æµ‹è¯•ï¼Œç›‘æ§ä»¥ä¸‹å‚æ•°ï¼š
- å‘åŠ¨æœºè½¬é€Ÿ
- æ²¹å‹æŒ‡ç¤º
- æ¸©åº¦æŒ‡ç¤º
- æŒ¯åŠ¨æ°´å¹³

ä»»åŠ¡2ï¼šèºæ—‹æ¡¨æ£€æŸ¥ç¨‹åº

è­¦å‘Šï¼šèºæ—‹æ¡¨æ£€æŸ¥æ—¶å¿…é¡»ç¡®ä¿èºæ—‹æ¡¨å®Œå…¨é™æ­¢ï¼Œå¹¶è®¾ç½®å®‰å…¨è­¦ç¤ºæ ‡å¿—ã€‚
""",

            'structured': """
# é¡¹ç›®ç®¡ç†è§„èŒƒæ–‡æ¡£

## 1. é¡¹ç›®ç”Ÿå‘½å‘¨æœŸç®¡ç†

### 1.1 é¡¹ç›®å¯åŠ¨é˜¶æ®µ
#### 1.1.1 éœ€æ±‚åˆ†æ
- ä¸šåŠ¡éœ€æ±‚æ”¶é›†
- æŠ€æœ¯éœ€æ±‚åˆ†æ
- å¯è¡Œæ€§ç ”ç©¶

#### 1.1.2 é¡¹ç›®è§„åˆ’
- é¡¹ç›®èŒƒå›´å®šä¹‰
- æ—¶é—´è®¡åˆ’åˆ¶å®š
- èµ„æºåˆ†é…è®¡åˆ’

### 1.2 é¡¹ç›®æ‰§è¡Œé˜¶æ®µ
#### 1.2.1 å¼€å‘ç®¡ç†
- ä»£ç å¼€å‘è§„èŒƒ
- ç‰ˆæœ¬æ§åˆ¶ç®¡ç†
- ä»£ç å®¡æŸ¥æµç¨‹

#### 1.2.2 è´¨é‡æ§åˆ¶
- å•å…ƒæµ‹è¯•è¦æ±‚
- é›†æˆæµ‹è¯•æµç¨‹
- æ€§èƒ½æµ‹è¯•æ ‡å‡†

## 2. å›¢é˜Ÿåä½œè§„èŒƒ

### 2.1 æ²Ÿé€šæœºåˆ¶
- æ—¥å¸¸ç«™ä¼šåˆ¶åº¦
- å‘¨æŠ¥æ±‡æŠ¥æœºåˆ¶
- æœˆåº¦æ€»ç»“ä¼šè®®

### 2.2 æ–‡æ¡£ç®¡ç†
- æŠ€æœ¯æ–‡æ¡£ç¼–å†™è§„èŒƒ
- æ–‡æ¡£ç‰ˆæœ¬æ§åˆ¶
- çŸ¥è¯†åº“ç»´æŠ¤
""",

            'semantic': """
äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•æ­£åœ¨æ·±åˆ»æ”¹å˜æˆ‘ä»¬çš„ä¸–ç•Œã€‚æœºå™¨å­¦ä¹ ä½œä¸ºäººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å’Œæ”¹è¿›ã€‚

æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚å®ƒæ¨¡ä»¿äººè„‘ç¥ç»ç½‘ç»œçš„ç»“æ„ï¼Œé€šè¿‡å¤šå±‚ç¥ç»ç½‘ç»œæ¥å¤„ç†å¤æ‚çš„æ•°æ®æ¨¡å¼ã€‚è¿™ç§æ–¹æ³•åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œè¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚

ç„¶è€Œï¼Œäººå·¥æ™ºèƒ½çš„å‘å±•ä¹Ÿå¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ã€‚æ•°æ®éšç§ã€ç®—æ³•åè§å’Œå°±ä¸šå½±å“ç­‰é—®é¢˜éœ€è¦æˆ‘ä»¬è®¤çœŸå¯¹å¾…ã€‚æˆ‘ä»¬å¿…é¡»åœ¨æ¨åŠ¨æŠ€æœ¯è¿›æ­¥çš„åŒæ—¶ï¼Œç¡®ä¿äººå·¥æ™ºèƒ½çš„å‘å±•ç¬¦åˆäººç±»çš„æ•´ä½“åˆ©ç›Šã€‚

å› æ­¤ï¼Œå»ºç«‹å®Œå–„çš„äººå·¥æ™ºèƒ½æ²»ç†æ¡†æ¶å˜å¾—è‡³å…³é‡è¦ã€‚è¿™éœ€è¦æ”¿åºœã€ä¼ä¸šå’Œå­¦æœ¯ç•Œçš„å…±åŒåŠªåŠ›ï¼Œåˆ¶å®šç›¸åº”çš„æ³•å¾‹æ³•è§„å’Œä¼¦ç†å‡†åˆ™ã€‚
""",

            'performance': """
ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–æ˜¯ä¸€ä¸ªæŒç»­çš„è¿‡ç¨‹ï¼Œéœ€è¦ä»å¤šä¸ªç»´åº¦è¿›è¡Œè€ƒè™‘å’Œå®æ–½ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å»ºç«‹å®Œå–„çš„æ€§èƒ½ç›‘æ§ä½“ç³»ï¼Œå®æ—¶æ”¶é›†ç³»ç»Ÿè¿è¡Œæ•°æ®ï¼ŒåŒ…æ‹¬CPUä½¿ç”¨ç‡ã€å†…å­˜å ç”¨ã€ç£ç›˜I/Oã€ç½‘ç»œå¸¦å®½ç­‰å…³é”®æŒ‡æ ‡ã€‚é€šè¿‡è¿™äº›æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥åŠæ—¶å‘ç°æ€§èƒ½ç“¶é¢ˆï¼Œå¹¶é‡‡å–ç›¸åº”çš„ä¼˜åŒ–æªæ–½ã€‚åœ¨æ•°æ®åº“å±‚é¢ï¼Œæˆ‘ä»¬éœ€è¦ä¼˜åŒ–æŸ¥è¯¢è¯­å¥ï¼Œå»ºç«‹åˆé€‚çš„ç´¢å¼•ï¼Œåˆç†è®¾è®¡è¡¨ç»“æ„ï¼Œå¹¶è€ƒè™‘è¯»å†™åˆ†ç¦»ã€åˆ†åº“åˆ†è¡¨ç­‰ç­–ç•¥ã€‚åœ¨åº”ç”¨å±‚é¢ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç¼“å­˜æœºåˆ¶ã€å¼‚æ­¥å¤„ç†ã€è¿æ¥æ± ä¼˜åŒ–ç­‰æ–¹å¼æå‡æ€§èƒ½ã€‚åŒæ—¶ï¼Œä»£ç å±‚é¢çš„ä¼˜åŒ–ä¹Ÿä¸å®¹å¿½è§†ï¼ŒåŒ…æ‹¬ç®—æ³•ä¼˜åŒ–ã€å†…å­˜ç®¡ç†ã€å¹¶å‘æ§åˆ¶ç­‰ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿæ¶æ„çš„åˆç†è®¾è®¡ä¹Ÿæ˜¯æ€§èƒ½ä¼˜åŒ–çš„é‡è¦å› ç´ ï¼Œå¾®æœåŠ¡æ¶æ„ã€è´Ÿè½½å‡è¡¡ã€CDNåŠ é€Ÿç­‰éƒ½èƒ½æœ‰æ•ˆæå‡ç³»ç»Ÿæ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬è¿˜éœ€è¦å»ºç«‹æ€§èƒ½æµ‹è¯•ä½“ç³»ï¼Œå®šæœŸè¿›è¡Œå‹åŠ›æµ‹è¯•å’Œæ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œç¡®ä¿ç³»ç»Ÿåœ¨å„ç§è´Ÿè½½æ¡ä»¶ä¸‹éƒ½èƒ½ç¨³å®šè¿è¡Œã€‚
"""
        }

        return samples.get(text_type, samples['general'])


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="ç®€åŒ–åˆ†å—ç³»ç»Ÿé¢„è®¾é…ç½®æµ‹è¯•è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  %(prog)s --demo                           # è¿è¡Œæ¼”ç¤ºæ¨¡å¼
  %(prog)s --list-presets                   # åˆ—å‡ºå¯ç”¨é¢„è®¾
  %(prog)s -i document.txt                  # æµ‹è¯•æ–‡ä»¶
  %(prog)s -t "æµ‹è¯•æ–‡æœ¬å†…å®¹"                 # æµ‹è¯•ç›´æ¥è¾“å…¥
  %(prog)s --performance                    # æ€§èƒ½æµ‹è¯•
  %(prog)s --compare -t "æµ‹è¯•æ–‡æœ¬"           # é¢„è®¾å¯¹æ¯”
  %(prog)s -p semantic --chunk-size 500    # è‡ªå®šä¹‰å‚æ•°

é¢„è®¾é…ç½®è¯´æ˜:
  quick              å¿«é€Ÿåˆ†å—é…ç½®ï¼ˆ500å­—ç¬¦ï¼‰
  standard           æ ‡å‡†åˆ†å—é…ç½®ï¼ˆ1000å­—ç¬¦ï¼‰
  semantic           è¯­ä¹‰åˆ†å—é…ç½®ï¼ˆæ›¿ä»£åŸsemantic_chunkerï¼‰
  structure          ç»“æ„åˆ†å—é…ç½®ï¼ˆæ›¿ä»£åŸstructure_chunkerï¼‰
  aviation_maintenance  èˆªç©ºç»´ä¿®æ–‡æ¡£é…ç½®
  aviation_regulation   èˆªç©ºè§„ç« é…ç½®
  aviation_standard     èˆªç©ºæ ‡å‡†é…ç½®
  aviation_training     èˆªç©ºåŸ¹è®­é…ç½®
  high_quality       é«˜è´¨é‡åˆ†å—é…ç½®
        """
    )

    # è¾“å…¥å‚æ•°
    input_group = parser.add_mutually_exclusive_group()
    input_group.add_argument('--input', '-i', help='è¾“å…¥æ–‡ä»¶è·¯å¾„')
    input_group.add_argument('--text', '-t', help='ç›´æ¥è¾“å…¥æ–‡æœ¬å†…å®¹')
    input_group.add_argument('--demo', action='store_true', help='è¿è¡Œæ¼”ç¤ºæ¨¡å¼')
    input_group.add_argument('--performance', action='store_true', help='æ€§èƒ½æµ‹è¯•æ¨¡å¼')
    input_group.add_argument('--list-presets', action='store_true', help='åˆ—å‡ºå¯ç”¨é¢„è®¾')

    # åˆ†å—å‚æ•°
    parser.add_argument('--preset', '-p', help='æŒ‡å®šé¢„è®¾é…ç½®åç§°')
    parser.add_argument('--chunk-size', type=int, default=1000, help='åˆ†å—å¤§å° (é»˜è®¤: 1000)')
    parser.add_argument('--chunk-overlap', type=int, default=200, help='é‡å å¤§å° (é»˜è®¤: 200)')
    parser.add_argument('--min-chunk-size', type=int, default=100, help='æœ€å°åˆ†å—å¤§å° (é»˜è®¤: 100)')
    parser.add_argument('--max-chunk-size', type=int, default=2000, help='æœ€å¤§åˆ†å—å¤§å° (é»˜è®¤: 2000)')

    # åŠŸèƒ½å‚æ•°
    parser.add_argument('--compare', action='store_true', help='å¯¹æ¯”ä¸åŒé¢„è®¾')
    parser.add_argument('--test-auto-selection', action='store_true', help='æµ‹è¯•è‡ªåŠ¨é¢„è®¾é€‰æ‹©')
    parser.add_argument('--test-quality-assessment', action='store_true', help='æµ‹è¯•è´¨é‡æ£€æµ‹åŠŸèƒ½')

    # è´¨é‡æ£€æµ‹å‚æ•°
    parser.add_argument('--quality-strategy', choices=['basic', 'strict', 'disabled'],
                       default='basic', help='è´¨é‡æ£€æµ‹ç­–ç•¥ (é»˜è®¤: basicï¼Œé€‰æ‹©disabledå¯ç¦ç”¨æ£€æµ‹)')

    # è¾“å‡ºå‚æ•°
    parser.add_argument('--output-format', choices=['detailed', 'simple', 'json'],
                       default='detailed', help='è¾“å‡ºæ ¼å¼ (é»˜è®¤: detailed)')
    parser.add_argument('--quiet', '-q', action='store_true', help='é™é»˜æ¨¡å¼ï¼Œåªè¾“å‡ºç»“æœ')

    args = parser.parse_args()

    # æ„å»ºé…ç½®
    config = {
        'chunk_size': args.chunk_size,
        'chunk_overlap': args.chunk_overlap,
        'min_chunk_size': args.min_chunk_size,
        'max_chunk_size': args.max_chunk_size,
        'preserve_context': True
    }
    
    # å¤„ç†è´¨é‡æ£€æµ‹é…ç½®
    config['quality_strategy'] = args.quality_strategy
    config['enable_quality_assessment'] = (args.quality_strategy != 'disabled')
    
    try:
        # åˆ›å»ºæµ‹è¯•å™¨
        tester = SimplifiedChunkingTester(config)

        if not args.quiet:
            print("ğŸš€ ç®€åŒ–åˆ†å—ç³»ç»Ÿé¢„è®¾é…ç½®æµ‹è¯•è„šæœ¬å¯åŠ¨")
            print(f"ğŸ“‹ å½“å‰é…ç½®: åˆ†å—å¤§å°={args.chunk_size}, é‡å ={args.chunk_overlap}")

        # æ ¹æ®å‚æ•°æ‰§è¡Œä¸åŒçš„æµ‹è¯•æ¨¡å¼
        if args.list_presets:
            tester.list_available_presets()
        elif args.demo:
            tester.run_demo()
        elif args.performance:
            tester.run_performance_test()
        elif args.test_auto_selection:
            # æµ‹è¯•è‡ªåŠ¨é¢„è®¾é€‰æ‹©
            test_cases = [
                {
                    'text': 'ç¬¬ä¸€ç«  å‘åŠ¨æœºç»´ä¿®ç¨‹åº\n\nä»»åŠ¡1ï¼šæ£€æŸ¥å‘åŠ¨æœº\næ­¥éª¤1ï¼šå…³é—­å‘åŠ¨æœº\nè­¦å‘Šï¼šæ³¨æ„å®‰å…¨',
                    'metadata': {'title': 'ç»´ä¿®æ‰‹å†Œ', 'document_type': 'manual'},
                    'expected_preset': 'aviation_maintenance'
                },
                {
                    'text': 'ç¬¬ä¸€æ¡ å®‰å…¨è§„å®š\nç¬¬äºŒæ¡ æ“ä½œè§„èŒƒ\nç¬¬ä¸‰æ¡ è´£ä»»æ¡æ¬¾',
                    'metadata': {'title': 'å®‰å…¨è§„ç« ', 'document_type': 'regulation'},
                    'expected_preset': 'aviation_regulation'
                },
                {
                    'text': '# æŠ€æœ¯æ ‡å‡†æ–‡æ¡£\n\n## è¦æ±‚1\nè§„æ ¼è¯´æ˜\n\n## æµ‹è¯•æ–¹æ³•\næµ‹è¯•ç¨‹åº',
                    'metadata': {'title': 'æŠ€æœ¯æ ‡å‡†', 'document_type': 'standard'},
                    'expected_preset': 'aviation_standard'
                },
                {
                    'text': 'å­¦ä¹ ç›®æ ‡ï¼šæŒæ¡åŸºæœ¬æ¦‚å¿µ\nçŸ¥è¯†ç‚¹1ï¼šç†è®ºåŸºç¡€\nç»ƒä¹ 1ï¼šå®è·µæ“ä½œ',
                    'metadata': {'title': 'åŸ¹è®­æ•™æ', 'document_type': 'training'},
                    'expected_preset': 'aviation_training'
                },
                {
                    'text': 'è¿™æ˜¯ä¸€ä¸ªæ™®é€šçš„æ–‡æ¡£å†…å®¹ã€‚åŒ…å«å¤šä¸ªæ®µè½å’Œå¥å­ã€‚',
                    'metadata': {'file_extension': '.txt'},
                    'expected_preset': 'semantic'
                }
            ]
            tester.test_automatic_preset_selection(test_cases)
        elif args.test_quality_assessment:
            # æµ‹è¯•è´¨é‡æ£€æµ‹åŠŸèƒ½
            if args.input:
                if not os.path.exists(args.input):
                    print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
                    sys.exit(1)
                with open(args.input, 'r', encoding='utf-8') as f:
                    text = f.read()
                metadata = {
                    'file_name': os.path.basename(args.input),
                    'file_path': args.input,
                    'document_type': 'user_document',
                    'title': f'ç”¨æˆ·æ–‡æ¡£: {os.path.basename(args.input)}'
                }
            elif args.text:
                text = args.text
                metadata = {
                    'file_name': 'direct_input.txt',
                    'document_type': 'direct_input',
                    'title': 'ç›´æ¥è¾“å…¥æ–‡æœ¬'
                }
            else:
                # ä½¿ç”¨é»˜è®¤æµ‹è¯•æ–‡æœ¬
                text = tester._get_sample_text('general')
                metadata = {
                    'file_name': 'quality_test.txt',
                    'document_type': 'quality_test',
                    'title': 'è´¨é‡æ£€æµ‹æµ‹è¯•æ–‡æ¡£'
                }

            tester.test_quality_assessment(text, metadata)
        elif args.compare and (args.input or args.text):
            # é¢„è®¾å¯¹æ¯”æ¨¡å¼
            if args.input:
                if not os.path.exists(args.input):
                    print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
                    sys.exit(1)
                with open(args.input, 'r', encoding='utf-8') as f:
                    text = f.read()
                metadata = {
                    'file_name': os.path.basename(args.input),
                    'file_path': args.input,
                    'document_type': 'user_document',
                    'title': f'ç”¨æˆ·æ–‡æ¡£: {os.path.basename(args.input)}'
                }
            else:
                text = args.text
                metadata = {
                    'file_name': 'direct_input.txt',
                    'document_type': 'direct_input',
                    'title': 'ç›´æ¥è¾“å…¥æ–‡æœ¬'
                }

            tester.compare_presets(text, metadata)
        elif args.input or args.text:
            # å•ä¸€æµ‹è¯•æ¨¡å¼
            if args.input:
                if not os.path.exists(args.input):
                    print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
                    sys.exit(1)
                with open(args.input, 'r', encoding='utf-8') as f:
                    text = f.read()
                metadata = {
                    'file_name': os.path.basename(args.input),
                    'file_path': args.input,
                    'document_type': 'user_document',
                    'title': f'ç”¨æˆ·æ–‡æ¡£: {os.path.basename(args.input)}'
                }
            else:
                text = args.text
                metadata = {
                    'file_name': 'direct_input.txt',
                    'document_type': 'direct_input',
                    'title': 'ç›´æ¥è¾“å…¥æ–‡æœ¬'
                }

            result = tester.test_preset_chunking(text, metadata, args.preset)
            tester.visualize_chunks(result, args.output_format)
        else:
            # é»˜è®¤æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
            parser.print_help()
            print("\nğŸ’¡ æç¤º:")
            print("  --demo                    è¿è¡Œæ¼”ç¤ºæ¨¡å¼")
            print("  --list-presets            æŸ¥çœ‹å¯ç”¨é¢„è®¾")
            print("  --test-auto-selection     æµ‹è¯•è‡ªåŠ¨é¢„è®¾é€‰æ‹©")
            print("  --test-quality-assessment æµ‹è¯•è´¨é‡æ£€æµ‹åŠŸèƒ½")
            print("  --help                    æŸ¥çœ‹è¯¦ç»†å¸®åŠ©")

    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
    except Exception as e:
        error_msg = f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}"
        print(f"\nâŒ {error_msg}")
        if 'tester' in locals():
            tester.logger.error(error_msg)
        if not args.quiet:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
