#!/usr/bin/env python3
"""
æ¨¡å—åç§°: test_chunking_complete
åŠŸèƒ½æè¿°: RAG Flowæ–‡æ¡£åˆ†å—åŠŸèƒ½å®Œæ•´æµ‹è¯•è„šæœ¬ï¼Œæ”¯æŒæ‰€æœ‰å¯ç”¨çš„åˆ†å—ç­–ç•¥
åˆ›å»ºæ—¥æœŸ: 2025-06-19
ä½œè€…: Sniperz
ç‰ˆæœ¬: v1.0.0

ä½¿ç”¨è¯´æ˜:
    python test_chunking_complete.py --demo                    # è¿è¡Œæ¼”ç¤ºæ¨¡å¼
    python test_chunking_complete.py -i document.txt          # æµ‹è¯•æ–‡ä»¶
    python test_chunking_complete.py -t "æµ‹è¯•æ–‡æœ¬"             # æµ‹è¯•ç›´æ¥è¾“å…¥çš„æ–‡æœ¬
    python test_chunking_complete.py --performance             # æ€§èƒ½æµ‹è¯•æ¨¡å¼
    python test_chunking_complete.py -s recursive --chunk-size 500  # è‡ªå®šä¹‰å‚æ•°
    python test_chunking_complete.py --list-strategies         # åˆ—å‡ºå¯ç”¨ç­–ç•¥

æ”¯æŒçš„åˆ†å—ç­–ç•¥ï¼ˆæ ¹æ®ç¯å¢ƒè‡ªåŠ¨æ£€æµ‹ï¼‰:
    - recursive: é€’å½’å­—ç¬¦åˆ†å—å™¨
    - semantic: è¯­ä¹‰åˆ†å—å™¨
    - structure: ç»“æ„åˆ†å—å™¨
    - aviation_maintenance: èˆªç©ºç»´ä¿®æ–‡æ¡£åˆ†å—å™¨
    - aviation_regulation: èˆªç©ºè§„ç« åˆ†å—å™¨
    - aviation_standard: èˆªç©ºæ ‡å‡†åˆ†å—å™¨
    - aviation_training: èˆªç©ºåŸ¹è®­åˆ†å—å™¨
"""

import argparse
import json
import os
import sys
import time
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥åŸºç¡€æ¨¡å—
try:
    from core.document_processor.chunking.chunking_engine import (
        ChunkingEngine, ChunkType, ChunkMetadata, TextChunk, ChunkingStrategy
    )
    CHUNKING_ENGINE_AVAILABLE = True
except ImportError as e:
    print(f"å¯¼å…¥ChunkingEngineå¤±è´¥: {e}")
    print("å°†ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬çš„æµ‹è¯•åŠŸèƒ½")
    CHUNKING_ENGINE_AVAILABLE = False

# å°è¯•å¯¼å…¥æ—¥å¿—ç®¡ç†å™¨
try:
    from utils.logger import SZ_LoggerManager
    USE_CUSTOM_LOGGER = True
except ImportError:
    USE_CUSTOM_LOGGER = False


class SafeChunkingEngine:
    """
    å®‰å…¨çš„åˆ†å—å¼•æ“åŒ…è£…å™¨
    
    èƒ½å¤Ÿä¼˜é›…åœ°å¤„ç†ç¼ºå¤±çš„ä¾èµ–ï¼ŒåªåŠ è½½å¯ç”¨çš„åˆ†å—ç­–ç•¥
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–å®‰å…¨åˆ†å—å¼•æ“
        
        Args:
            config: åˆ†å—å¼•æ“é…ç½®å‚æ•°
        """
        self.config = config or {}
        
        # è®¾ç½®æ—¥å¿—è®°å½•å™¨
        if USE_CUSTOM_LOGGER:
            self.logger = SZ_LoggerManager.setup_logger(
                logger_name="safe_chunking_engine",
                log_file="chunking_test.log",
                level=logging.INFO
            )
        else:
            logging.basicConfig(level=logging.INFO)
            self.logger = logging.getLogger("safe_chunking_engine")
        
        self.strategies = {}
        self.available_strategies = []
        
        if CHUNKING_ENGINE_AVAILABLE:
            try:
                # ä½¿ç”¨å®Œæ•´çš„ChunkingEngine
                self.engine = ChunkingEngine(self.config)
                self.available_strategies = self.engine.get_available_strategies()
                self.logger.info(f"æˆåŠŸåŠ è½½ChunkingEngineï¼Œå¯ç”¨ç­–ç•¥: {self.available_strategies}")
            except Exception as e:
                self.logger.error(f"ChunkingEngineåˆå§‹åŒ–å¤±è´¥: {e}")
                self.engine = None
        else:
            self.engine = None
            self.logger.warning("ChunkingEngineä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ç®€åŒ–æ¨¡å¼")
    
    def chunk_document(self, text_content: str, document_metadata: Dict[str, Any],
                      strategy_name: Optional[str] = None) -> List:
        """
        æ‰§è¡Œæ–‡æ¡£åˆ†å—
        
        Args:
            text_content: æ–‡æ¡£æ–‡æœ¬å†…å®¹
            document_metadata: æ–‡æ¡£å…ƒæ•°æ®
            strategy_name: æŒ‡å®šçš„åˆ†å—ç­–ç•¥åç§°
            
        Returns:
            list: åˆ†å—ç»“æœåˆ—è¡¨
        """
        if self.engine:
            return self.engine.chunk_document(text_content, document_metadata, strategy_name)
        else:
            # ç®€åŒ–æ¨¡å¼ï¼šåŸºæœ¬çš„æ–‡æœ¬åˆ†å—
            return self._simple_chunk(text_content, document_metadata)
    
    def _simple_chunk(self, text: str, metadata: Dict[str, Any]) -> List:
        """
        ç®€åŒ–çš„æ–‡æœ¬åˆ†å—å®ç°ï¼Œæ”¯æŒRecursiveCharacterChunkerçš„åŸºæœ¬åŠŸèƒ½

        Args:
            text: å¾…åˆ†å—çš„æ–‡æœ¬
            metadata: æ–‡æ¡£å…ƒæ•°æ®

        Returns:
            list: ç®€åŒ–çš„åˆ†å—ç»“æœ
        """
        chunk_size = self.config.get('chunk_size', 1000)
        chunk_overlap = self.config.get('chunk_overlap', 200)
        separators = self.config.get('separators', ['\n\n', '\n', 'ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?', 'ï¼›', ';', 'ï¼Œ', ',', ' '])
        keep_separator = self.config.get('keep_separator', True)
        is_separator_regex = self.config.get('is_separator_regex', False)
        strip_whitespace = self.config.get('strip_whitespace', True)

        # é€’å½’åˆ†å—å‡½æ•°
        def _split_text_with_separators(text: str, separators: List[str]) -> List[str]:
            """ä½¿ç”¨åˆ†éš”ç¬¦é€’å½’åˆ†å‰²æ–‡æœ¬"""
            if not separators or len(text) <= chunk_size:
                return [text] if text.strip() else []

            separator = separators[0]
            remaining_separators = separators[1:]

            # åˆ†å‰²æ–‡æœ¬
            if is_separator_regex:
                import re
                parts = re.split(separator, text)
            else:
                parts = text.split(separator)

            # é‡æ–°ç»„åˆåˆ†å—
            chunks = []
            current_chunk = ""

            for i, part in enumerate(parts):
                # æ·»åŠ åˆ†éš”ç¬¦ï¼ˆå¦‚æœéœ€è¦ä¿ç•™ï¼‰
                if keep_separator and i > 0:
                    if is_separator_regex:
                        # å¯¹äºæ­£åˆ™è¡¨è¾¾å¼ï¼Œæˆ‘ä»¬æ— æ³•å‡†ç¡®æ¢å¤åŸå§‹åˆ†éš”ç¬¦ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå­—ç¬¦ä½œä¸ºè¿‘ä¼¼
                        current_chunk += separator[0] if separator else ""
                    else:
                        current_chunk += separator

                # æ£€æŸ¥æ·»åŠ è¿™éƒ¨åˆ†åæ˜¯å¦è¶…è¿‡å¤§å°é™åˆ¶
                potential_chunk = current_chunk + part

                if len(potential_chunk) <= chunk_size:
                    current_chunk = potential_chunk
                else:
                    # å¦‚æœå½“å‰å—ä¸ä¸ºç©ºï¼Œå…ˆä¿å­˜å®ƒ
                    if current_chunk.strip():
                        chunks.append(current_chunk)

                    # å¦‚æœè¿™éƒ¨åˆ†æœ¬èº«å°±å¤ªå¤§ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
                    if len(part) > chunk_size:
                        sub_chunks = _split_text_with_separators(part, remaining_separators)
                        chunks.extend(sub_chunks)
                        current_chunk = ""
                    else:
                        current_chunk = part

            # æ·»åŠ æœ€åçš„å—
            if current_chunk.strip():
                chunks.append(current_chunk)

            return chunks

        # æ‰§è¡Œåˆ†å—
        text_chunks = _split_text_with_separators(text, separators)

        # å¤„ç†é‡å 
        final_chunks = []
        for i, chunk_text in enumerate(text_chunks):
            if strip_whitespace:
                chunk_text = chunk_text.strip()

            if not chunk_text:
                continue

            # è®¡ç®—åœ¨åŸæ–‡ä¸­çš„ä½ç½®
            start_pos = text.find(chunk_text) if i == 0 else None
            end_pos = start_pos + len(chunk_text) if start_pos is not None else None

            # å¤„ç†é‡å å†…å®¹
            overlap_content = None
            if i > 0 and chunk_overlap > 0:
                prev_chunk = text_chunks[i-1]
                if len(prev_chunk) > chunk_overlap:
                    overlap_content = prev_chunk[-chunk_overlap:]

            # åˆ›å»ºåˆ†å—å¯¹è±¡
            chunk = {
                'content': chunk_text,
                'character_count': len(chunk_text),
                'word_count': len(chunk_text.split()),
                'quality_score': 0.8,  # é»˜è®¤è´¨é‡è¯„åˆ†
                'overlap_content': overlap_content,
                'metadata': {
                    'chunk_id': f"simple_{i:04d}",
                    'chunk_type': 'paragraph',
                    'start_position': start_pos,
                    'end_position': end_pos,
                    'source_document': metadata.get('file_name', 'unknown')
                }
            }

            final_chunks.append(chunk)

        return final_chunks
    
    def get_available_strategies(self) -> List[str]:
        """è·å–å¯ç”¨çš„åˆ†å—ç­–ç•¥åˆ—è¡¨"""
        if self.engine:
            return self.engine.get_available_strategies()
        else:
            return ['simple']
    
    def get_strategy_info(self, strategy_name: str) -> Dict[str, Any]:
        """è·å–ç­–ç•¥ä¿¡æ¯"""
        if self.engine:
            return self.engine.get_strategy_info(strategy_name)
        else:
            if strategy_name == 'simple':
                return {
                    'name': 'simple',
                    'class_name': 'SimpleChunker',
                    'strategy_name': 'simple',
                    'description': 'ç®€åŒ–çš„æ–‡æœ¬åˆ†å—å™¨ï¼Œç”¨äºæµ‹è¯•ç¯å¢ƒ'
                }
            else:
                return {'error': f'ç­–ç•¥ä¸å­˜åœ¨: {strategy_name}'}
    
    def validate_chunks(self, chunks: List) -> Dict[str, Any]:
        """éªŒè¯åˆ†å—ç»“æœ"""
        if self.engine and hasattr(self.engine, 'validate_chunks'):
            # å¦‚æœchunksæ˜¯ç®€åŒ–æ ¼å¼ï¼Œéœ€è¦è½¬æ¢
            if chunks and isinstance(chunks[0], dict):
                # ç®€åŒ–æ ¼å¼ï¼Œç›´æ¥è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                return self._simple_validate(chunks)
            else:
                return self.engine.validate_chunks(chunks)
        else:
            return self._simple_validate(chunks)
    
    def _simple_validate(self, chunks: List) -> Dict[str, Any]:
        """ç®€åŒ–çš„åˆ†å—éªŒè¯"""
        if not chunks:
            return {'total_chunks': 0, 'valid_chunks': 0, 'invalid_chunks': 0}
        
        total_chars = 0
        quality_scores = []
        min_size = float('inf')
        max_size = 0
        
        for chunk in chunks:
            if isinstance(chunk, dict):
                char_count = chunk.get('character_count', 0)
                quality = chunk.get('quality_score', 0.5)
            else:
                char_count = getattr(chunk, 'character_count', 0)
                quality = getattr(chunk, 'quality_score', 0.5)
            
            total_chars += char_count
            quality_scores.append(quality)
            min_size = min(min_size, char_count)
            max_size = max(max_size, char_count)
        
        return {
            'total_chunks': len(chunks),
            'valid_chunks': len(chunks),
            'invalid_chunks': 0,
            'quality_scores': quality_scores,
            'avg_quality_score': sum(quality_scores) / len(quality_scores) if quality_scores else 0,
            'size_distribution': {
                'min_size': min_size if min_size != float('inf') else 0,
                'max_size': max_size,
                'avg_size': total_chars / len(chunks) if chunks else 0
            },
            'issues': []
        }


class ChunkingTester:
    """
    æ–‡æ¡£åˆ†å—æµ‹è¯•å™¨
    
    æä¾›å…¨é¢çš„æ–‡æ¡£åˆ†å—åŠŸèƒ½æµ‹è¯•ï¼ŒåŒ…æ‹¬ï¼š
    - å¤šç§åˆ†å—ç­–ç•¥æµ‹è¯•
    - åˆ†å—æ•ˆæœå¯è§†åŒ–
    - æ€§èƒ½ç»Ÿè®¡åˆ†æ
    - å‚æ•°è°ƒä¼˜å»ºè®®
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–æµ‹è¯•å™¨
        
        Args:
            config: åˆ†å—å¼•æ“é…ç½®å‚æ•°
        """
        self.config = config or {}
        
        # è®¾ç½®æ—¥å¿—è®°å½•å™¨
        if USE_CUSTOM_LOGGER:
            self.logger = SZ_LoggerManager.setup_logger(
                logger_name="chunking_tester",
                log_file="chunking_test.log",
                level=logging.INFO
            )
        else:
            logging.basicConfig(level=logging.INFO)
            self.logger = logging.getLogger("chunking_tester")
        
        try:
            # åˆ›å»ºå®‰å…¨çš„åˆ†å—å¼•æ“
            self.engine = SafeChunkingEngine(self.config)
            self.logger.info("åˆ†å—æµ‹è¯•å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            self.logger.error(f"åˆ†å—æµ‹è¯•å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def test_chunking(self, text: str, metadata: Dict[str, Any], 
                     strategy_name: Optional[str] = None) -> Dict[str, Any]:
        """
        æ‰§è¡Œåˆ†å—æµ‹è¯•
        
        Args:
            text: å¾…åˆ†å—çš„æ–‡æœ¬
            metadata: æ–‡æ¡£å…ƒæ•°æ®
            strategy_name: æŒ‡å®šçš„åˆ†å—ç­–ç•¥åç§°
            
        Returns:
            dict: æµ‹è¯•ç»“æœï¼ŒåŒ…å«åˆ†å—ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯
        """
        try:
            start_time = time.time()
            
            # æ‰§è¡Œåˆ†å—
            chunks = self.engine.chunk_document(text, metadata, strategy_name)
            
            processing_time = time.time() - start_time
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            stats = self._calculate_statistics(chunks, processing_time, len(text))
            
            # éªŒè¯åˆ†å—ç»“æœ
            validation = self.engine.validate_chunks(chunks)
            
            return {
                'chunks': chunks,
                'statistics': stats,
                'validation': validation,
                'processing_time': processing_time,
                'strategy_used': strategy_name or 'auto'
            }
            
        except Exception as e:
            self.logger.error(f"åˆ†å—æµ‹è¯•å¤±è´¥: {e}")
            raise
    
    def _calculate_statistics(self, chunks: List, processing_time: float, 
                            original_length: int) -> Dict[str, Any]:
        """
        è®¡ç®—åˆ†å—ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            chunks: åˆ†å—ç»“æœåˆ—è¡¨
            processing_time: å¤„ç†æ—¶é—´
            original_length: åŸæ–‡é•¿åº¦
            
        Returns:
            dict: ç»Ÿè®¡ä¿¡æ¯
        """
        if not chunks:
            return {
                'chunk_count': 0,
                'total_characters': 0,
                'average_chunk_size': 0,
                'min_chunk_size': 0,
                'max_chunk_size': 0,
                'processing_speed': 0,
                'coverage_rate': 0
            }
        
        # å¤„ç†ä¸åŒæ ¼å¼çš„chunks
        chunk_sizes = []
        total_chars = 0
        
        for chunk in chunks:
            if isinstance(chunk, dict):
                char_count = chunk.get('character_count', 0)
            else:
                char_count = getattr(chunk, 'character_count', 0)
            
            chunk_sizes.append(char_count)
            total_chars += char_count
        
        return {
            'chunk_count': len(chunks),
            'total_characters': total_chars,
            'average_chunk_size': total_chars / len(chunks),
            'min_chunk_size': min(chunk_sizes) if chunk_sizes else 0,
            'max_chunk_size': max(chunk_sizes) if chunk_sizes else 0,
            'processing_speed': original_length / processing_time if processing_time > 0 else 0,
            'coverage_rate': (total_chars / original_length) * 100 if original_length > 0 else 0
        }
    
    def list_available_strategies(self) -> None:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„åˆ†å—ç­–ç•¥"""
        print("\n" + "="*80)
        print("ğŸ“‹ å¯ç”¨çš„åˆ†å—ç­–ç•¥")
        print("="*80)

        strategies = self.engine.get_available_strategies()

        if not strategies:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„åˆ†å—ç­–ç•¥")
            return

        for strategy in strategies:
            info = self.engine.get_strategy_info(strategy)
            print(f"\nğŸ”¸ {strategy}")
            print(f"   ç±»å: {info.get('class_name', 'æœªçŸ¥')}")
            print(f"   æè¿°: {info.get('description', 'æ— æè¿°')}")

            if 'error' in info:
                print(f"   âŒ é”™è¯¯: {info['error']}")

    def show_recursive_separators(self) -> None:
        """æ˜¾ç¤ºRecursiveCharacterChunkerçš„é»˜è®¤åˆ†éš”ç¬¦åˆ—è¡¨"""
        print("\n" + "="*80)
        print("ğŸ“ RecursiveCharacterChunker é»˜è®¤åˆ†éš”ç¬¦åˆ—è¡¨")
        print("="*80)

        # è·å–é»˜è®¤åˆ†éš”ç¬¦åˆ—è¡¨
        default_separators = [
            # æ®µè½åˆ†éš”ç¬¦
            "\\n\\n", "\\n\\n\\n",

            # ä¸­æ–‡æ®µè½æ ‡è®°
            "\\nç¬¬", "\\nç« ", "\\nèŠ‚", "\\næ¡",

            # è‹±æ–‡æ®µè½æ ‡è®°
            "\\nChapter", "\\nSection", "\\nArticle",

            # åˆ—è¡¨å’Œç¼–å·
            "\\n\\nâ€¢", "\\n\\n-", "\\n\\n*", "\\n\\n1.", "\\n\\n2.", "\\n\\n3.",

            # å•è¡Œåˆ†éš”ç¬¦
            "\\n",

            # å¥å­åˆ†éš”ç¬¦
            "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?",

            # å­å¥åˆ†éš”ç¬¦
            "ï¼›", ";", "ï¼Œ", ",",

            # è¯è¯­åˆ†éš”ç¬¦
            " ", "\\t",

            # ä¸­æ–‡æ ‡ç‚¹
            "ã€", "ï¼š", ":",

            # é›¶å®½å­—ç¬¦
            "\\u200b", "\\uff0c", "\\u3001", "\\uff0e", "\\u3002",

            # æœ€åçš„å›é€€é€‰é¡¹
            '""'
        ]

        print("ğŸ“Œ åˆ†éš”ç¬¦æŒ‰ä¼˜å…ˆçº§ä»é«˜åˆ°ä½æ’åºï¼š")
        print("\nğŸ”¹ æ®µè½çº§åˆ†éš”ç¬¦:")
        for sep in default_separators[:13]:
            print(f"   '{sep}'")

        print("\nğŸ”¹ å¥å­çº§åˆ†éš”ç¬¦:")
        for sep in default_separators[13:19]:
            print(f"   '{sep}'")

        print("\nğŸ”¹ å­å¥çº§åˆ†éš”ç¬¦:")
        for sep in default_separators[19:23]:
            print(f"   '{sep}'")

        print("\nğŸ”¹ è¯è¯­çº§åˆ†éš”ç¬¦:")
        for sep in default_separators[23:28]:
            print(f"   '{sep}'")

        print("\nğŸ”¹ ç‰¹æ®Šå­—ç¬¦:")
        for sep in default_separators[28:]:
            print(f"   '{sep}'")

        print(f"\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
        print(f"   â€¢ åˆ†å—å™¨ä¼šæŒ‰ä¼˜å…ˆçº§ä¾æ¬¡å°è¯•è¿™äº›åˆ†éš”ç¬¦")
        print(f"   â€¢ å¦‚æœä½¿ç”¨æŸä¸ªåˆ†éš”ç¬¦åˆ†å‰²åçš„ç‰‡æ®µä»ç„¶å¤ªå¤§ï¼Œä¼šå°è¯•ä¸‹ä¸€ä¸ªåˆ†éš”ç¬¦")
        print(f"   â€¢ å¯ä»¥ä½¿ç”¨ --separators å‚æ•°è‡ªå®šä¹‰åˆ†éš”ç¬¦åˆ—è¡¨")
        print(f"   â€¢ ä½¿ç”¨ --is-separator-regex å¯ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼")
        print(f"   â€¢ ä½¿ç”¨ --no-keep-separator ä¸ä¿ç•™åˆ†éš”ç¬¦")
    
    def compare_strategies(self, text: str, metadata: Dict[str, Any]) -> None:
        """
        æ¯”è¾ƒä¸åŒç­–ç•¥çš„åˆ†å—æ•ˆæœ
        
        Args:
            text: å¾…åˆ†å—çš„æ–‡æœ¬
            metadata: æ–‡æ¡£å…ƒæ•°æ®
        """
        print("\n" + "="*80)
        print("ğŸ” åˆ†å—ç­–ç•¥å¯¹æ¯”åˆ†æ")
        print("="*80)
        
        strategies = self.engine.get_available_strategies()
        results = {}
        
        for strategy in strategies:
            print(f"\næµ‹è¯•ç­–ç•¥: {strategy}")
            try:
                result = self.test_chunking(text, metadata, strategy)
                results[strategy] = result
                
                stats = result['statistics']
                print(f"  åˆ†å—æ•°é‡: {stats['chunk_count']}")
                print(f"  å¤„ç†æ—¶é—´: {result['processing_time']:.3f}ç§’")
                print(f"  å¹³å‡å¤§å°: {stats['average_chunk_size']:.1f}å­—ç¬¦")
                print(f"  è´¨é‡è¯„åˆ†: {result['validation'].get('avg_quality_score', 0):.3f}")
                
            except Exception as e:
                print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
        
        # è¾“å‡ºå¯¹æ¯”æ€»ç»“
        if len(results) > 1:
            print(f"\nğŸ“Š å¯¹æ¯”æ€»ç»“:")
            print(f"{'ç­–ç•¥':>15} {'åˆ†å—æ•°':>8} {'æ—¶é—´(s)':>10} {'å¹³å‡å¤§å°':>10} {'è´¨é‡':>8}")
            print("-" * 60)
            
            for strategy, result in results.items():
                stats = result['statistics']
                quality = result['validation'].get('avg_quality_score', 0)
                print(f"{strategy:>15} {stats['chunk_count']:>8} "
                      f"{result['processing_time']:>9.3f} {stats['average_chunk_size']:>9.1f} "
                      f"{quality:>7.3f}")

    def visualize_chunks(self, result: Dict[str, Any], output_format: str = 'detailed') -> None:
        """
        å¯è§†åŒ–åˆ†å—ç»“æœ

        Args:
            result: æµ‹è¯•ç»“æœ
            output_format: è¾“å‡ºæ ¼å¼ ('detailed', 'simple', 'json')
        """
        chunks = result['chunks']
        stats = result['statistics']
        validation = result['validation']

        if output_format == 'json':
            self._output_json(result)
            return

        # è¾“å‡ºæ ‡é¢˜
        print("\n" + "="*80)
        print(f"ğŸ” RAG Flow æ–‡æ¡£åˆ†å—æµ‹è¯•ç»“æœ")
        print(f"ğŸ“Š ç­–ç•¥: {result['strategy_used']}")
        print(f"â±ï¸  å¤„ç†æ—¶é—´: {result['processing_time']:.3f}ç§’")
        print("="*80)

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        self._print_statistics(stats, validation)

        if output_format == 'detailed':
            self._print_detailed_chunks(chunks)
        else:
            self._print_simple_chunks(chunks)

    def _print_statistics(self, stats: Dict[str, Any], validation: Dict[str, Any]) -> None:
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   åˆ†å—æ•°é‡: {stats['chunk_count']}")
        print(f"   æ€»å­—ç¬¦æ•°: {stats['total_characters']}")
        print(f"   å¹³å‡åˆ†å—å¤§å°: {stats['average_chunk_size']:.1f} å­—ç¬¦")
        print(f"   æœ€å°åˆ†å—: {stats['min_chunk_size']} å­—ç¬¦")
        print(f"   æœ€å¤§åˆ†å—: {stats['max_chunk_size']} å­—ç¬¦")
        print(f"   å¤„ç†é€Ÿåº¦: {stats['processing_speed']:.0f} å­—ç¬¦/ç§’")
        print(f"   è¦†ç›–ç‡: {stats['coverage_rate']:.1f}%")

        # éªŒè¯ä¿¡æ¯
        print(f"\nğŸ” è´¨é‡éªŒè¯:")
        print(f"   æœ‰æ•ˆåˆ†å—: {validation['valid_chunks']}")
        print(f"   æ— æ•ˆåˆ†å—: {validation['invalid_chunks']}")
        print(f"   å¹³å‡è´¨é‡è¯„åˆ†: {validation.get('avg_quality_score', 0):.3f}")

        if validation.get('issues'):
            print(f"   âš ï¸  å‘ç°é—®é¢˜: {len(validation['issues'])}ä¸ª")
            for issue in validation['issues'][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªé—®é¢˜
                print(f"      - {issue}")
            if len(validation['issues']) > 3:
                print(f"      - ... è¿˜æœ‰{len(validation['issues']) - 3}ä¸ªé—®é¢˜")

    def _print_detailed_chunks(self, chunks: List) -> None:
        """æ‰“å°è¯¦ç»†åˆ†å—ä¿¡æ¯"""
        print(f"\nğŸ“ è¯¦ç»†åˆ†å—ç»“æœ:")

        for i, chunk in enumerate(chunks, 1):
            print(f"\n--- åˆ†å— {i} ---")

            # å¤„ç†ä¸åŒæ ¼å¼çš„chunk
            if isinstance(chunk, dict):
                char_count = chunk.get('character_count', 0)
                word_count = chunk.get('word_count', 0)
                quality_score = chunk.get('quality_score', 0)
                content = chunk.get('content', '')
                overlap_content = chunk.get('overlap_content')
                metadata = chunk.get('metadata', {})
            else:
                char_count = getattr(chunk, 'character_count', 0)
                word_count = getattr(chunk, 'word_count', 0)
                quality_score = getattr(chunk, 'quality_score', 0)
                content = getattr(chunk, 'content', '')
                overlap_content = getattr(chunk, 'overlap_content', None)
                metadata = getattr(chunk, 'metadata', {})

            print(f"å¤§å°: {char_count} å­—ç¬¦ | è¯æ•°: {word_count}")
            print(f"è´¨é‡è¯„åˆ†: {quality_score:.3f}")

            # æ˜¾ç¤ºä½ç½®ä¿¡æ¯
            if isinstance(metadata, dict):
                start_pos = metadata.get('start_position')
                end_pos = metadata.get('end_position')
            else:
                start_pos = getattr(metadata, 'start_position', None)
                end_pos = getattr(metadata, 'end_position', None)

            if start_pos is not None and end_pos is not None:
                print(f"ä½ç½®: {start_pos}-{end_pos}")

            # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
            content_preview = content[:200] + "..." if len(content) > 200 else content
            print(f"å†…å®¹: {content_preview}")

            # æ˜¾ç¤ºé‡å å†…å®¹
            if overlap_content:
                overlap_preview = overlap_content[:100] + "..." if len(overlap_content) > 100 else overlap_content
                print(f"é‡å : {overlap_preview}")

    def _print_simple_chunks(self, chunks: List) -> None:
        """æ‰“å°ç®€æ´åˆ†å—ä¿¡æ¯"""
        print(f"\nğŸ“‹ åˆ†å—æ¦‚è§ˆ:")

        for i, chunk in enumerate(chunks, 1):
            # å¤„ç†ä¸åŒæ ¼å¼çš„chunk
            if isinstance(chunk, dict):
                char_count = chunk.get('character_count', 0)
                quality_score = chunk.get('quality_score', 0)
                content = chunk.get('content', '')
            else:
                char_count = getattr(chunk, 'character_count', 0)
                quality_score = getattr(chunk, 'quality_score', 0)
                content = getattr(chunk, 'content', '')

            content_preview = content[:50] + "..." if len(content) > 50 else content
            quality_info = f" (è´¨é‡: {quality_score:.2f})" if quality_score > 0 else ""
            print(f"  {i:2d}. [{char_count:4d}å­—ç¬¦] {content_preview}{quality_info}")

    def _output_json(self, result: Dict[str, Any]) -> None:
        """è¾“å‡ºJSONæ ¼å¼ç»“æœ"""
        # è½¬æ¢chunksä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        serializable_chunks = []
        for chunk in result['chunks']:
            if isinstance(chunk, dict):
                chunk_data = chunk.copy()
            else:
                chunk_data = {
                    'content': getattr(chunk, 'content', ''),
                    'character_count': getattr(chunk, 'character_count', 0),
                    'word_count': getattr(chunk, 'word_count', 0),
                    'quality_score': getattr(chunk, 'quality_score', 0.0),
                    'overlap_content': getattr(chunk, 'overlap_content', None),
                    'metadata': {}
                }

                # å¤„ç†metadata
                metadata = getattr(chunk, 'metadata', None)
                if metadata:
                    if isinstance(metadata, dict):
                        chunk_data['metadata'] = metadata
                    else:
                        chunk_data['metadata'] = {
                            'chunk_id': getattr(metadata, 'chunk_id', ''),
                            'chunk_type': str(getattr(metadata, 'chunk_type', '')),
                            'start_position': getattr(metadata, 'start_position', None),
                            'end_position': getattr(metadata, 'end_position', None)
                        }

            serializable_chunks.append(chunk_data)

        output = {
            'strategy_used': result['strategy_used'],
            'processing_time': result['processing_time'],
            'statistics': result['statistics'],
            'validation': result['validation'],
            'chunks': serializable_chunks
        }

        print(json.dumps(output, ensure_ascii=False, indent=2))

    def run_performance_test(self, text_sizes: List[int] = None) -> None:
        """
        è¿è¡Œæ€§èƒ½æµ‹è¯•

        Args:
            text_sizes: æµ‹è¯•æ–‡æœ¬å¤§å°åˆ—è¡¨ï¼ˆå­—ç¬¦æ•°ï¼‰
        """
        if text_sizes is None:
            text_sizes = [1000, 5000, 10000, 50000, 100000]

        print("\n" + "="*80)
        print("ğŸš€ æ€§èƒ½æµ‹è¯•æ¨¡å¼")
        print("="*80)

        # ç”Ÿæˆæµ‹è¯•æ–‡æœ¬
        base_text = self._get_sample_text('performance')

        results = []

        for size in text_sizes:
            # ç”ŸæˆæŒ‡å®šå¤§å°çš„æ–‡æœ¬
            test_text = (base_text * (size // len(base_text) + 1))[:size]

            metadata = {
                'file_name': f'performance_test_{size}.txt',
                'document_type': 'performance_test',
                'title': f'æ€§èƒ½æµ‹è¯•æ–‡æ¡£ ({size}å­—ç¬¦)'
            }

            print(f"\næµ‹è¯•æ–‡æœ¬å¤§å°: {size:,} å­—ç¬¦")

            try:
                result = self.test_chunking(test_text, metadata)
                results.append({
                    'size': size,
                    'time': result['processing_time'],
                    'chunks': result['statistics']['chunk_count'],
                    'speed': result['statistics']['processing_speed']
                })

                print(f"  å¤„ç†æ—¶é—´: {result['processing_time']:.3f}ç§’")
                print(f"  åˆ†å—æ•°é‡: {result['statistics']['chunk_count']}")
                print(f"  å¤„ç†é€Ÿåº¦: {result['statistics']['processing_speed']:.0f} å­—ç¬¦/ç§’")

            except Exception as e:
                print(f"  æµ‹è¯•å¤±è´¥: {e}")

        # è¾“å‡ºæ€§èƒ½æ€»ç»“
        if results:
            print(f"\nğŸ“Š æ€§èƒ½æµ‹è¯•æ€»ç»“:")
            print(f"{'æ–‡æœ¬å¤§å°':>10} {'å¤„ç†æ—¶é—´':>10} {'åˆ†å—æ•°':>8} {'é€Ÿåº¦':>12}")
            print("-" * 45)
            for r in results:
                print(f"{r['size']:>10,} {r['time']:>9.3f}s {r['chunks']:>7} {r['speed']:>10.0f}/s")

    def run_demo(self) -> None:
        """è¿è¡Œæ¼”ç¤ºæ¨¡å¼"""
        print("\n" + "="*80)
        print("ğŸ¯ RAG Flow æ–‡æ¡£åˆ†å—åŠŸèƒ½æ¼”ç¤º")
        print("="*80)

        demo_scenarios = [
            ('é€šç”¨æŠ€æœ¯æ–‡æ¡£', 'general'),
            ('èˆªç©ºç»´ä¿®æ‰‹å†Œ', 'aviation'),
            ('ä»£ç æ–‡æ¡£', 'code'),
            ('ç»“æ„åŒ–æ–‡æ¡£', 'structured')
        ]

        for name, text_type in demo_scenarios:
            print(f"\nğŸ”¸ æ¼”ç¤ºåœºæ™¯: {name}")
            print("-" * 40)

            text = self._get_sample_text(text_type)
            metadata = {
                'file_name': f'{text_type}_demo.txt',
                'document_type': text_type,
                'title': name
            }

            try:
                result = self.test_chunking(text, metadata)
                self.visualize_chunks(result, 'simple')
            except Exception as e:
                print(f"æ¼”ç¤ºå¤±è´¥: {e}")

        # æ·»åŠ RecursiveCharacterChunkeré«˜çº§åŠŸèƒ½æ¼”ç¤º
        self._demo_recursive_features()

    def _demo_recursive_features(self) -> None:
        """æ¼”ç¤ºRecursiveCharacterChunkerçš„é«˜çº§åŠŸèƒ½"""
        print("\n" + "="*80)
        print("ğŸ”§ RecursiveCharacterChunker é«˜çº§åŠŸèƒ½æ¼”ç¤º")
        print("="*80)

        demo_text = "ç¬¬ä¸€ç« ï¼šç³»ç»Ÿæ¶æ„ã€‚æœ¬ç« ä»‹ç»ç³»ç»Ÿçš„æ•´ä½“æ¶æ„è®¾è®¡ã€‚ç¬¬äºŒç« ï¼šæŠ€æœ¯é€‰å‹ï¼æœ¬ç« è¯¦ç»†è¯´æ˜å„ç§æŠ€æœ¯çš„é€‰æ‹©ç†ç”±ã€‚ç¬¬ä¸‰ç« ï¼šå®æ–½æ–¹æ¡ˆï¼Ÿæœ¬ç« æè¿°å…·ä½“çš„å®æ–½æ­¥éª¤å’Œæ³¨æ„äº‹é¡¹ã€‚"

        demo_configs = [
            {
                'name': 'é»˜è®¤é…ç½®',
                'config': {'chunk_size': 30, 'chunk_overlap': 5},
                'description': 'ä½¿ç”¨é»˜è®¤åˆ†éš”ç¬¦å’Œé…ç½®'
            },
            {
                'name': 'è‡ªå®šä¹‰åˆ†éš”ç¬¦',
                'config': {'chunk_size': 30, 'chunk_overlap': 5, 'separators': ['ã€‚', 'ï¼', 'ï¼Ÿ']},
                'description': 'åªä½¿ç”¨ä¸­æ–‡å¥å·ã€æ„Ÿå¹å·ã€é—®å·ä½œä¸ºåˆ†éš”ç¬¦'
            },
            {
                'name': 'ä¸ä¿ç•™åˆ†éš”ç¬¦',
                'config': {'chunk_size': 30, 'chunk_overlap': 5, 'separators': ['ã€‚', 'ï¼', 'ï¼Ÿ'], 'keep_separator': False},
                'description': 'åˆ†å—æ—¶ä¸ä¿ç•™åˆ†éš”ç¬¦'
            },
            {
                'name': 'æ®µè½çº§åˆ†éš”ç¬¦',
                'config': {'chunk_size': 50, 'chunk_overlap': 10, 'separators': ['ç¬¬', 'ç« ', 'ï¼š']},
                'description': 'ä½¿ç”¨ç« èŠ‚æ ‡è¯†ç¬¦è¿›è¡Œåˆ†å—'
            }
        ]

        for demo in demo_configs:
            print(f"\nğŸ”¹ {demo['name']}")
            print(f"   {demo['description']}")
            print("   " + "-" * 50)

            # ä¸´æ—¶ä¿®æ”¹é…ç½®
            original_config = self.config.copy()
            self.config.update(demo['config'])

            # é‡æ–°åˆ›å»ºå¼•æ“
            temp_engine = SafeChunkingEngine(self.config)

            try:
                chunks = temp_engine.chunk_document(demo_text, {'file_name': 'demo.txt'})

                print(f"   åˆ†å—æ•°é‡: {len(chunks)}")
                for i, chunk in enumerate(chunks, 1):
                    content = chunk['content'] if isinstance(chunk, dict) else chunk.content
                    char_count = chunk['character_count'] if isinstance(chunk, dict) else chunk.character_count
                    print(f"   {i}. [{char_count:2d}å­—ç¬¦] {content}")

            except Exception as e:
                print(f"   âŒ æ¼”ç¤ºå¤±è´¥: {e}")
            finally:
                # æ¢å¤åŸå§‹é…ç½®
                self.config = original_config

    def _get_sample_text(self, text_type: str) -> str:
        """è·å–ç¤ºä¾‹æ–‡æœ¬"""
        samples = {
            'general': """
ç¬¬ä¸€ç«  ç³»ç»Ÿæ¶æ„è®¾è®¡

1.1 æ¦‚è¿°
æœ¬ç³»ç»Ÿé‡‡ç”¨å¾®æœåŠ¡æ¶æ„è®¾è®¡ï¼Œå…·æœ‰é«˜å¯ç”¨æ€§ã€å¯æ‰©å±•æ€§å’Œå¯ç»´æŠ¤æ€§çš„ç‰¹ç‚¹ã€‚ç³»ç»Ÿä¸»è¦ç”±ä»¥ä¸‹å‡ ä¸ªæ ¸å¿ƒæ¨¡å—ç»„æˆï¼šç”¨æˆ·ç®¡ç†æ¨¡å—ã€æ•°æ®å¤„ç†æ¨¡å—ã€æ¥å£æœåŠ¡æ¨¡å—å’Œç›‘æ§æ¨¡å—ã€‚

1.2 æŠ€æœ¯é€‰å‹
åœ¨æŠ€æœ¯é€‰å‹æ–¹é¢ï¼Œæˆ‘ä»¬é€‰æ‹©äº†ä»¥ä¸‹æŠ€æœ¯æ ˆï¼š
- åç«¯æ¡†æ¶ï¼šSpring Boot 2.7
- æ•°æ®åº“ï¼šMySQL 8.0 + Redis 6.2
- æ¶ˆæ¯é˜Ÿåˆ—ï¼šRabbitMQ 3.9
- å®¹å™¨åŒ–ï¼šDocker + Kubernetes
- ç›‘æ§ï¼šPrometheus + Grafana

1.3 ç³»ç»Ÿç‰¹æ€§
ç³»ç»Ÿå…·å¤‡ä»¥ä¸‹æ ¸å¿ƒç‰¹æ€§ï¼š
1. é«˜å¹¶å‘å¤„ç†èƒ½åŠ›ï¼Œæ”¯æŒæ¯ç§’10ä¸‡æ¬¡è¯·æ±‚
2. æ•°æ®ä¸€è‡´æ€§ä¿è¯ï¼Œé‡‡ç”¨åˆ†å¸ƒå¼äº‹åŠ¡ç®¡ç†
3. è‡ªåŠ¨æ•…éšœæ¢å¤ï¼Œå…·å¤‡å®Œå–„çš„å®¹é”™æœºåˆ¶
4. å®æ—¶ç›‘æ§å‘Šè­¦ï¼Œç¡®ä¿ç³»ç»Ÿç¨³å®šè¿è¡Œ
""",

            'aviation': """
ä»»åŠ¡1ï¼šå‘åŠ¨æœºæ—¥å¸¸æ£€æŸ¥ç¨‹åº

è­¦å‘Šï¼šåœ¨è¿›è¡Œä»»ä½•å‘åŠ¨æœºæ£€æŸ¥å‰ï¼Œå¿…é¡»ç¡®ä¿å‘åŠ¨æœºå®Œå…¨å†·å´ï¼Œå¹¶æ–­å¼€æ‰€æœ‰ç”µæºã€‚

æ­¥éª¤1ï¼šå¤–è§‚æ£€æŸ¥
æ£€æŸ¥å‘åŠ¨æœºå¤–å£³æ˜¯å¦æœ‰è£‚çº¹ã€è…èš€æˆ–å¼‚å¸¸ç£¨æŸã€‚ç‰¹åˆ«æ³¨æ„ä»¥ä¸‹éƒ¨ä½ï¼š
- è¿›æ°”é“å’Œæ’æ°”å£
- ç‡ƒæ²¹ç®¡è·¯è¿æ¥å¤„
- ç”µæ°”çº¿æŸå›ºå®šç‚¹
- å†·å´ç³»ç»Ÿç®¡è·¯

æ­¥éª¤2ï¼šæ¶²ä½æ£€æŸ¥
æ£€æŸ¥å„ç§æ¶²ä½“çš„æ¶²ä½æ˜¯å¦åœ¨æ­£å¸¸èŒƒå›´å†…ï¼š
- å‘åŠ¨æœºæœºæ²¹æ¶²ä½
- å†·å´æ¶²æ¶²ä½
- æ¶²å‹æ²¹æ¶²ä½

æ³¨æ„ï¼šæ‰€æœ‰æ¶²ä½æ£€æŸ¥å¿…é¡»åœ¨å‘åŠ¨æœºæ°´å¹³çŠ¶æ€ä¸‹è¿›è¡Œã€‚

æ­¥éª¤3ï¼šåŠŸèƒ½æµ‹è¯•
å¯åŠ¨å‘åŠ¨æœºè¿›è¡ŒåŠŸèƒ½æµ‹è¯•ï¼Œç›‘æ§ä»¥ä¸‹å‚æ•°ï¼š
- å‘åŠ¨æœºè½¬é€Ÿ
- æ²¹å‹æŒ‡ç¤º
- æ¸©åº¦æŒ‡ç¤º
- æŒ¯åŠ¨æ°´å¹³

ä»»åŠ¡2ï¼šèºæ—‹æ¡¨æ£€æŸ¥ç¨‹åº

è­¦å‘Šï¼šèºæ—‹æ¡¨æ£€æŸ¥æ—¶å¿…é¡»ç¡®ä¿èºæ—‹æ¡¨å®Œå…¨é™æ­¢ï¼Œå¹¶è®¾ç½®å®‰å…¨è­¦ç¤ºæ ‡å¿—ã€‚
""",

            'code': """
# ç”¨æˆ·è®¤è¯æ¨¡å—

## æ¦‚è¿°
æœ¬æ¨¡å—æä¾›å®Œæ•´çš„ç”¨æˆ·è®¤è¯åŠŸèƒ½ï¼ŒåŒ…æ‹¬ç™»å½•ã€æ³¨å†Œã€å¯†ç é‡ç½®ç­‰æ ¸å¿ƒåŠŸèƒ½ã€‚

## ä¸»è¦ç±»å’Œæ–¹æ³•

### UserAuthenticatorç±»
```python
class UserAuthenticator:
    def __init__(self, config):
        self.config = config
        self.session_manager = SessionManager()

    def authenticate(self, username, password):
        \"\"\"
        ç”¨æˆ·è®¤è¯æ–¹æ³•

        Args:
            username (str): ç”¨æˆ·å
            password (str): å¯†ç 

        Returns:
            bool: è®¤è¯ç»“æœ
        \"\"\"
        user = self.get_user(username)
        if user and self.verify_password(password, user.password_hash):
            return self.create_session(user)
        return False
```

### é…ç½®è¯´æ˜
ç³»ç»Ÿæ”¯æŒä»¥ä¸‹é…ç½®å‚æ•°ï¼š
- SESSION_TIMEOUT: ä¼šè¯è¶…æ—¶æ—¶é—´ï¼ˆé»˜è®¤30åˆ†é’Ÿï¼‰
- PASSWORD_MIN_LENGTH: å¯†ç æœ€å°é•¿åº¦ï¼ˆé»˜è®¤8ä½ï¼‰
- MAX_LOGIN_ATTEMPTS: æœ€å¤§ç™»å½•å°è¯•æ¬¡æ•°ï¼ˆé»˜è®¤5æ¬¡ï¼‰
""",

            'structured': """
# é¡¹ç›®ç®¡ç†è§„èŒƒæ–‡æ¡£

## 1. é¡¹ç›®ç”Ÿå‘½å‘¨æœŸç®¡ç†

### 1.1 é¡¹ç›®å¯åŠ¨é˜¶æ®µ
#### 1.1.1 éœ€æ±‚åˆ†æ
- ä¸šåŠ¡éœ€æ±‚æ”¶é›†
- æŠ€æœ¯éœ€æ±‚åˆ†æ
- å¯è¡Œæ€§ç ”ç©¶

#### 1.1.2 é¡¹ç›®è§„åˆ’
- é¡¹ç›®èŒƒå›´å®šä¹‰
- æ—¶é—´è®¡åˆ’åˆ¶å®š
- èµ„æºåˆ†é…è®¡åˆ’

### 1.2 é¡¹ç›®æ‰§è¡Œé˜¶æ®µ
#### 1.2.1 å¼€å‘ç®¡ç†
- ä»£ç å¼€å‘è§„èŒƒ
- ç‰ˆæœ¬æ§åˆ¶ç®¡ç†
- ä»£ç å®¡æŸ¥æµç¨‹

#### 1.2.2 è´¨é‡æ§åˆ¶
- å•å…ƒæµ‹è¯•è¦æ±‚
- é›†æˆæµ‹è¯•æµç¨‹
- æ€§èƒ½æµ‹è¯•æ ‡å‡†

## 2. å›¢é˜Ÿåä½œè§„èŒƒ

### 2.1 æ²Ÿé€šæœºåˆ¶
- æ—¥å¸¸ç«™ä¼šåˆ¶åº¦
- å‘¨æŠ¥æ±‡æŠ¥æœºåˆ¶
- æœˆåº¦æ€»ç»“ä¼šè®®

### 2.2 æ–‡æ¡£ç®¡ç†
- æŠ€æœ¯æ–‡æ¡£ç¼–å†™è§„èŒƒ
- æ–‡æ¡£ç‰ˆæœ¬æ§åˆ¶
- çŸ¥è¯†åº“ç»´æŠ¤
""",

            'performance': """
ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–æ˜¯ä¸€ä¸ªæŒç»­çš„è¿‡ç¨‹ï¼Œéœ€è¦ä»å¤šä¸ªç»´åº¦è¿›è¡Œè€ƒè™‘å’Œå®æ–½ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å»ºç«‹å®Œå–„çš„æ€§èƒ½ç›‘æ§ä½“ç³»ï¼Œå®æ—¶æ”¶é›†ç³»ç»Ÿè¿è¡Œæ•°æ®ï¼ŒåŒ…æ‹¬CPUä½¿ç”¨ç‡ã€å†…å­˜å ç”¨ã€ç£ç›˜I/Oã€ç½‘ç»œå¸¦å®½ç­‰å…³é”®æŒ‡æ ‡ã€‚é€šè¿‡è¿™äº›æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥åŠæ—¶å‘ç°æ€§èƒ½ç“¶é¢ˆï¼Œå¹¶é‡‡å–ç›¸åº”çš„ä¼˜åŒ–æªæ–½ã€‚åœ¨æ•°æ®åº“å±‚é¢ï¼Œæˆ‘ä»¬éœ€è¦ä¼˜åŒ–æŸ¥è¯¢è¯­å¥ï¼Œå»ºç«‹åˆé€‚çš„ç´¢å¼•ï¼Œåˆç†è®¾è®¡è¡¨ç»“æ„ï¼Œå¹¶è€ƒè™‘è¯»å†™åˆ†ç¦»ã€åˆ†åº“åˆ†è¡¨ç­‰ç­–ç•¥ã€‚åœ¨åº”ç”¨å±‚é¢ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç¼“å­˜æœºåˆ¶ã€å¼‚æ­¥å¤„ç†ã€è¿æ¥æ± ä¼˜åŒ–ç­‰æ–¹å¼æå‡æ€§èƒ½ã€‚åŒæ—¶ï¼Œä»£ç å±‚é¢çš„ä¼˜åŒ–ä¹Ÿä¸å®¹å¿½è§†ï¼ŒåŒ…æ‹¬ç®—æ³•ä¼˜åŒ–ã€å†…å­˜ç®¡ç†ã€å¹¶å‘æ§åˆ¶ç­‰ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿæ¶æ„çš„åˆç†è®¾è®¡ä¹Ÿæ˜¯æ€§èƒ½ä¼˜åŒ–çš„é‡è¦å› ç´ ï¼Œå¾®æœåŠ¡æ¶æ„ã€è´Ÿè½½å‡è¡¡ã€CDNåŠ é€Ÿç­‰éƒ½èƒ½æœ‰æ•ˆæå‡ç³»ç»Ÿæ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬è¿˜éœ€è¦å»ºç«‹æ€§èƒ½æµ‹è¯•ä½“ç³»ï¼Œå®šæœŸè¿›è¡Œå‹åŠ›æµ‹è¯•å’Œæ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œç¡®ä¿ç³»ç»Ÿåœ¨å„ç§è´Ÿè½½æ¡ä»¶ä¸‹éƒ½èƒ½ç¨³å®šè¿è¡Œã€‚
"""
        }

        return samples.get(text_type, samples['general'])


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="RAG Flowæ–‡æ¡£åˆ†å—åŠŸèƒ½å®Œæ•´æµ‹è¯•è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  %(prog)s --demo                           # è¿è¡Œæ¼”ç¤ºæ¨¡å¼
  %(prog)s --list-strategies                # åˆ—å‡ºå¯ç”¨ç­–ç•¥
  %(prog)s --show-separators               # æ˜¾ç¤ºé€’å½’åˆ†å—å™¨çš„é»˜è®¤åˆ†éš”ç¬¦
  %(prog)s -i document.txt                  # æµ‹è¯•æ–‡ä»¶
  %(prog)s -t "æµ‹è¯•æ–‡æœ¬å†…å®¹"                 # æµ‹è¯•ç›´æ¥è¾“å…¥
  %(prog)s --performance                    # æ€§èƒ½æµ‹è¯•
  %(prog)s --compare -t "æµ‹è¯•æ–‡æœ¬"           # ç­–ç•¥å¯¹æ¯”
  %(prog)s -s recursive --chunk-size 500   # è‡ªå®šä¹‰å‚æ•°

RecursiveCharacterChunker é«˜çº§ç”¨æ³•:
  %(prog)s -t "æ–‡æœ¬" --separators "ã€‚" "ï¼" "ï¼Ÿ"  # è‡ªå®šä¹‰åˆ†éš”ç¬¦
  %(prog)s -t "æ–‡æœ¬" --is-separator-regex        # å¯ç”¨æ­£åˆ™è¡¨è¾¾å¼
  %(prog)s -t "æ–‡æœ¬" --no-keep-separator         # ä¸ä¿ç•™åˆ†éš”ç¬¦
  %(prog)s -t "æ–‡æœ¬" --add-start-index           # æ·»åŠ ä½ç½®ç´¢å¼•
        """
    )

    # è¾“å…¥å‚æ•°
    input_group = parser.add_mutually_exclusive_group()
    input_group.add_argument('--input', '-i', help='è¾“å…¥æ–‡ä»¶è·¯å¾„')
    input_group.add_argument('--text', '-t', help='ç›´æ¥è¾“å…¥æ–‡æœ¬å†…å®¹')
    input_group.add_argument('--demo', action='store_true', help='è¿è¡Œæ¼”ç¤ºæ¨¡å¼')
    input_group.add_argument('--performance', action='store_true', help='æ€§èƒ½æµ‹è¯•æ¨¡å¼')
    input_group.add_argument('--list-strategies', action='store_true', help='åˆ—å‡ºå¯ç”¨ç­–ç•¥')

    # åˆ†å—å‚æ•°
    parser.add_argument('--strategy', '-s', help='æŒ‡å®šåˆ†å—ç­–ç•¥')
    parser.add_argument('--chunk-size', type=int, default=1000, help='åˆ†å—å¤§å° (é»˜è®¤: 1000)')
    parser.add_argument('--chunk-overlap', type=int, default=200, help='é‡å å¤§å° (é»˜è®¤: 200)')
    parser.add_argument('--min-chunk-size', type=int, default=100, help='æœ€å°åˆ†å—å¤§å° (é»˜è®¤: 100)')
    parser.add_argument('--max-chunk-size', type=int, default=2000, help='æœ€å¤§åˆ†å—å¤§å° (é»˜è®¤: 2000)')

    # RecursiveCharacterChunker ç‰¹æœ‰å‚æ•°
    parser.add_argument('--separators', nargs='*', help='è‡ªå®šä¹‰åˆ†éš”ç¬¦åˆ—è¡¨ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰')
    parser.add_argument('--is-separator-regex', action='store_true', help='åˆ†éš”ç¬¦æ˜¯å¦ä¸ºæ­£åˆ™è¡¨è¾¾å¼')
    parser.add_argument('--keep-separator', action='store_true', default=True, help='æ˜¯å¦ä¿ç•™åˆ†éš”ç¬¦ï¼ˆé»˜è®¤: Trueï¼‰')
    parser.add_argument('--no-keep-separator', action='store_true', help='ä¸ä¿ç•™åˆ†éš”ç¬¦')
    parser.add_argument('--add-start-index', action='store_true', help='æ·»åŠ èµ·å§‹ç´¢å¼•ä¿¡æ¯')
    parser.add_argument('--no-strip-whitespace', action='store_true', help='ä¸å»é™¤ç©ºç™½å­—ç¬¦')
    parser.add_argument('--show-separators', action='store_true', help='æ˜¾ç¤ºé»˜è®¤åˆ†éš”ç¬¦åˆ—è¡¨')

    # åŠŸèƒ½å‚æ•°
    parser.add_argument('--compare', action='store_true', help='å¯¹æ¯”ä¸åŒç­–ç•¥')
    parser.add_argument('--validate', action='store_true', help='è¯¦ç»†éªŒè¯åˆ†å—ç»“æœ')

    # è¾“å‡ºå‚æ•°
    parser.add_argument('--output-format', choices=['detailed', 'simple', 'json'],
                       default='detailed', help='è¾“å‡ºæ ¼å¼ (é»˜è®¤: detailed)')
    parser.add_argument('--quiet', '-q', action='store_true', help='é™é»˜æ¨¡å¼ï¼Œåªè¾“å‡ºç»“æœ')

    args = parser.parse_args()

    # æ„å»ºé…ç½®
    config = {
        'chunk_size': args.chunk_size,
        'chunk_overlap': args.chunk_overlap,
        'min_chunk_size': args.min_chunk_size,
        'max_chunk_size': args.max_chunk_size,
        'preserve_context': True
    }

    # æ·»åŠ  RecursiveCharacterChunker ç‰¹æœ‰é…ç½®
    if args.separators:
        config['separators'] = args.separators
    if args.is_separator_regex:
        config['is_separator_regex'] = True
    if args.no_keep_separator:
        config['keep_separator'] = False
    elif args.keep_separator:
        config['keep_separator'] = True
    if args.add_start_index:
        config['add_start_index'] = True
    if args.no_strip_whitespace:
        config['strip_whitespace'] = False

    try:
        # åˆ›å»ºæµ‹è¯•å™¨
        tester = ChunkingTester(config)

        if not args.quiet:
            print("ğŸš€ RAG Flow æ–‡æ¡£åˆ†å—å®Œæ•´æµ‹è¯•è„šæœ¬å¯åŠ¨")
            print(f"ğŸ“‹ å½“å‰é…ç½®: åˆ†å—å¤§å°={args.chunk_size}, é‡å ={args.chunk_overlap}")

        # æ ¹æ®å‚æ•°æ‰§è¡Œä¸åŒçš„æµ‹è¯•æ¨¡å¼
        if args.show_separators:
            tester.show_recursive_separators()
        elif args.list_strategies:
            tester.list_available_strategies()
        elif args.demo:
            tester.run_demo()
        elif args.performance:
            tester.run_performance_test()
        elif args.compare and (args.input or args.text):
            # ç­–ç•¥å¯¹æ¯”æ¨¡å¼
            if args.input:
                if not os.path.exists(args.input):
                    print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
                    sys.exit(1)
                with open(args.input, 'r', encoding='utf-8') as f:
                    text = f.read()
                metadata = {
                    'file_name': os.path.basename(args.input),
                    'file_path': args.input,
                    'document_type': 'user_document',
                    'title': f'ç”¨æˆ·æ–‡æ¡£: {os.path.basename(args.input)}'
                }
            else:
                text = args.text
                metadata = {
                    'file_name': 'direct_input.txt',
                    'document_type': 'direct_input',
                    'title': 'ç›´æ¥è¾“å…¥æ–‡æœ¬'
                }

            tester.compare_strategies(text, metadata)

        elif args.input or args.text:
            # å•ä¸€æµ‹è¯•æ¨¡å¼
            if args.input:
                if not os.path.exists(args.input):
                    print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
                    sys.exit(1)
                with open(args.input, 'r', encoding='utf-8') as f:
                    text = f.read()
                metadata = {
                    'file_name': os.path.basename(args.input),
                    'file_path': args.input,
                    'document_type': 'user_document',
                    'title': f'ç”¨æˆ·æ–‡æ¡£: {os.path.basename(args.input)}'
                }
            else:
                text = args.text
                metadata = {
                    'file_name': 'direct_input.txt',
                    'document_type': 'direct_input',
                    'title': 'ç›´æ¥è¾“å…¥æ–‡æœ¬'
                }

            result = tester.test_chunking(text, metadata, args.strategy)
            tester.visualize_chunks(result, args.output_format)

            if args.validate:
                print("\n" + "="*80)
                print("ğŸ” è¯¦ç»†éªŒè¯ç»“æœ")
                print("="*80)
                validation = result['validation']
                print(json.dumps(validation, ensure_ascii=False, indent=2))
        else:
            # é»˜è®¤æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
            parser.print_help()
            print("\nğŸ’¡ æç¤º:")
            print("  --demo              è¿è¡Œæ¼”ç¤ºæ¨¡å¼")
            print("  --list-strategies   æŸ¥çœ‹å¯ç”¨ç­–ç•¥")
            print("  --show-separators   æŸ¥çœ‹é€’å½’åˆ†å—å™¨åˆ†éš”ç¬¦")
            print("  --help              æŸ¥çœ‹è¯¦ç»†å¸®åŠ©")

    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        if not args.quiet:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
