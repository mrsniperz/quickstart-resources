#!/usr/bin/env python3
"""
ç®€åŒ–çš„åˆ†å—è´¨é‡è¯„ä¼°æµ‹è¯•

åŠŸèƒ½æè¿°: ç›´æ¥æµ‹è¯•è´¨é‡è¯„ä¼°æ–¹æ³•ï¼Œé¿å…ä¾èµ–é—®é¢˜
åˆ›å»ºæ—¥æœŸ: 2024-01-15
ä½œè€…: Sniperz
ç‰ˆæœ¬: v1.0.0
"""

import sys
import os
import logging
from dataclasses import dataclass
from typing import Optional, List, Dict, Any
from enum import Enum

# ç®€åŒ–çš„æ•°æ®ç»“æ„å®šä¹‰
class ChunkType(Enum):
    """åˆ†å—ç±»å‹æšä¸¾"""
    PARAGRAPH = "paragraph"
    SECTION = "section"
    CHAPTER = "chapter"
    LIST = "list"
    TABLE = "table"
    CODE = "code"
    MAINTENANCE_MANUAL = "maintenance_manual"
    REGULATION = "regulation"
    TECHNICAL_STANDARD = "technical_standard"
    TRAINING_MATERIAL = "training_material"
    OPERATION_PROCEDURE = "operation_procedure"

@dataclass
class ChunkMetadata:
    """åˆ†å—å…ƒæ•°æ®"""
    chunk_id: str
    chunk_type: ChunkType
    source_document: str
    page_number: Optional[int] = None
    section_title: Optional[str] = None
    start_position: Optional[int] = None
    end_position: Optional[int] = None
    parent_chunk_id: Optional[str] = None
    child_chunk_ids: List[str] = None
    confidence_score: float = 1.0
    processing_timestamp: Optional[str] = None

@dataclass
class TextChunk:
    """æ–‡æœ¬åˆ†å—æ•°æ®ç±»"""
    content: str
    metadata: ChunkMetadata
    word_count: int = 0
    character_count: int = 0
    overlap_content: Optional[str] = None
    quality_score: float = 0.0

# ç®€åŒ–çš„è´¨é‡è¯„ä¼°å™¨ç±»
class QualityAssessmentEngine:
    """è´¨é‡è¯„ä¼°å¼•æ“"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.logger = logging.getLogger(__name__)
        
        # é…ç½®å‚æ•°
        self.chunk_size = self.config.get('chunk_size', 1000)
        self.min_chunk_size = self.config.get('min_chunk_size', 100)
        self.max_chunk_size = self.config.get('max_chunk_size', 2000)
    
    def _get_quality_weights(self, metadata: ChunkMetadata) -> Dict[str, float]:
        """æ ¹æ®æ–‡æ¡£ç±»å‹è·å–è´¨é‡è¯„ä¼°æƒé‡é…ç½®"""
        try:
            doc_type = getattr(metadata, 'chunk_type', None)
            if hasattr(doc_type, 'value'):
                doc_type = doc_type.value
            
            weight_configs = {
                'maintenance_manual': {
                    'aviation_specific': 0.30,
                    'semantic_completeness': 0.25,
                    'information_density': 0.20,
                    'structure_quality': 0.20,
                    'size_appropriateness': 0.05
                },
                'regulation': {
                    'aviation_specific': 0.20,
                    'semantic_completeness': 0.30,
                    'information_density': 0.25,
                    'structure_quality': 0.20,
                    'size_appropriateness': 0.05
                },
                'technical_standard': {
                    'aviation_specific': 0.25,
                    'semantic_completeness': 0.25,
                    'information_density': 0.25,
                    'structure_quality': 0.20,
                    'size_appropriateness': 0.05
                },
                'training_material': {
                    'aviation_specific': 0.20,
                    'semantic_completeness': 0.30,
                    'information_density': 0.20,
                    'structure_quality': 0.25,
                    'size_appropriateness': 0.05
                }
            }
            
            default_weights = {
                'aviation_specific': 0.25,
                'semantic_completeness': 0.25,
                'information_density': 0.25,
                'structure_quality': 0.20,
                'size_appropriateness': 0.05
            }
            
            return weight_configs.get(str(doc_type), default_weights)
            
        except Exception as e:
            self.logger.warning(f"è·å–æƒé‡é…ç½®å¤±è´¥: {e}")
            return {
                'aviation_specific': 0.25,
                'semantic_completeness': 0.25,
                'information_density': 0.25,
                'structure_quality': 0.20,
                'size_appropriateness': 0.05
            }
    
    def _calculate_aviation_specific_score(self, chunk: TextChunk) -> float:
        """è®¡ç®—èˆªç©ºé¢†åŸŸç‰¹å®šæ€§è¯„åˆ†"""
        try:
            score = 0.5  # ä»è¾ƒä½çš„åŸºç¡€åˆ†å¼€å§‹
            content = chunk.content.lower()

            # èˆªç©ºæœ¯è¯­å¯†åº¦æ£€æŸ¥
            aviation_terms = [
                'å‘åŠ¨æœº', 'æ¶²å‹ç³»ç»Ÿ', 'ç‡ƒæ²¹ç³»ç»Ÿ', 'ç”µæ°”ç³»ç»Ÿ', 'èµ·è½æ¶',
                'é£è¡Œæ§åˆ¶', 'å¯¼èˆªç³»ç»Ÿ', 'é€šä¿¡ç³»ç»Ÿ', 'å®¢èˆ±', 'è´§èˆ±',
                'engine', 'hydraulic', 'fuel system', 'electrical', 'landing gear',
                'flight control', 'navigation', 'communication', 'cabin', 'cargo'
            ]

            # è®¡ç®—èˆªç©ºæœ¯è¯­å¯†åº¦
            aviation_term_count = sum(1 for term in aviation_terms if term in content)
            if aviation_term_count > 0:
                score += min(0.3, aviation_term_count * 0.1)  # æ¯ä¸ªæœ¯è¯­åŠ 0.1åˆ†ï¼Œæœ€å¤š0.3åˆ†

            # æ£€æŸ¥èˆªç©ºæœ¯è¯­æ˜¯å¦è¢«æˆªæ–­
            for term in aviation_terms:
                if term in content:
                    if content.startswith(term[1:]) or content.endswith(term[:-1]):
                        score -= 0.3  # æœ¯è¯­æˆªæ–­ä¸¥é‡æ‰£åˆ†
                        break

            # å®‰å…¨ä¿¡æ¯å®Œæ•´æ€§æ£€æŸ¥
            safety_keywords = [
                'è­¦å‘Š', 'æ³¨æ„', 'å±é™©', 'ç¦æ­¢', 'å¿…é¡»',
                'warning', 'caution', 'danger', 'prohibited', 'must'
            ]

            safety_found = any(keyword in content for keyword in safety_keywords)
            if safety_found:
                score += 0.2  # åŒ…å«å®‰å…¨ä¿¡æ¯åŠ åˆ†
                if not self._is_safety_info_complete(chunk.content):
                    score -= 0.4  # å®‰å…¨ä¿¡æ¯ä¸å®Œæ•´ä¸¥é‡æ‰£åˆ†

            # æ“ä½œæ­¥éª¤è¿è´¯æ€§æ£€æŸ¥
            import re
            step_patterns = [
                r'æ­¥éª¤\s*\d+', r'ç¬¬\s*\d+\s*æ­¥', r'step\s+\d+',
                r'\d+\.\s', r'\(\d+\)', r'[a-z]\)'
            ]

            has_steps = any(re.search(pattern, content, re.IGNORECASE) for pattern in step_patterns)
            if has_steps:
                score += 0.2  # åŒ…å«æ­¥éª¤åŠ åˆ†
                if self._has_incomplete_procedures(chunk.content):
                    score -= 0.3  # æ­¥éª¤ä¸å®Œæ•´æ‰£åˆ†

            # æŠ€æœ¯å‚æ•°æ£€æŸ¥
            param_patterns = [
                r'\d+\s*(rpm|psi|Â°c|Â°f|kg|lb|ft|m|v|a|bar|mpa)',
                r'å‹åŠ›[:ï¼š]\s*\d+', r'æ¸©åº¦[:ï¼š]\s*\d+', r'è½¬é€Ÿ[:ï¼š]\s*\d+'
            ]

            has_params = any(re.search(pattern, content, re.IGNORECASE) for pattern in param_patterns)
            if has_params:
                score += 0.2  # åŒ…å«æŠ€æœ¯å‚æ•°åŠ åˆ†

            return max(0.0, min(1.0, score))

        except Exception as e:
            self.logger.warning(f"èˆªç©ºç‰¹å®šæ€§è¯„åˆ†è®¡ç®—å¤±è´¥: {e}")
            return 0.5
    
    def _is_safety_info_complete(self, content: str) -> bool:
        """æ£€æŸ¥å®‰å…¨ä¿¡æ¯æ˜¯å¦å®Œæ•´"""
        try:
            safety_start_patterns = ['è­¦å‘Š:', 'æ³¨æ„:', 'å±é™©:', 'WARNING:', 'CAUTION:', 'DANGER:']

            for pattern in safety_start_patterns:
                if pattern in content:
                    start_idx = content.find(pattern)
                    after_warning = content[start_idx + len(pattern):].strip()

                    # æ›´ä¸¥æ ¼çš„å®Œæ•´æ€§æ£€æŸ¥
                    if len(after_warning) < 20:  # æé«˜æœ€å°é•¿åº¦è¦æ±‚
                        return False

                    # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„å¥å­ç»“æ„
                    if not any(after_warning.endswith(end) for end in ['.', 'ã€‚', '!', 'ï¼']):
                        return False

                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å…·ä½“çš„å®‰å…¨æªæ–½æè¿°
                    safety_action_keywords = ['å¿…é¡»', 'ç¦æ­¢', 'åº”è¯¥', 'ä¸å¾—', 'must', 'should', 'do not', 'never']
                    if not any(keyword in after_warning for keyword in safety_action_keywords):
                        return False

            return True

        except Exception:
            return True
    
    def _has_incomplete_procedures(self, content: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰ä¸å®Œæ•´çš„æ“ä½œæ­¥éª¤"""
        try:
            import re
            
            step_numbers = re.findall(r'æ­¥éª¤\s*(\d+)|ç¬¬\s*(\d+)\s*æ­¥|step\s+(\d+)|^(\d+)\.', content, re.IGNORECASE | re.MULTILINE)
            
            if not step_numbers:
                return False
            
            numbers = []
            for match in step_numbers:
                for group in match:
                    if group:
                        numbers.append(int(group))
                        break
            
            if not numbers:
                return False
            
            numbers.sort()
            for i in range(len(numbers) - 1):
                if numbers[i + 1] - numbers[i] > 1:
                    return True
            
            if numbers and not content.strip().endswith(('.', 'ã€‚', 'å®Œæˆ', 'complete', 'done')):
                return True
            
            return False
            
        except Exception:
            return False
    
    def _calculate_semantic_completeness_score(self, chunk: TextChunk) -> float:
        """è®¡ç®—è¯­ä¹‰å®Œæ•´æ€§è¯„åˆ†"""
        try:
            score = 0.6  # ä»è¾ƒä½çš„åŸºç¡€åˆ†å¼€å§‹
            content = chunk.content.strip()

            # æ£€æŸ¥å†…å®¹ç»“æŸçš„å®Œæ•´æ€§
            proper_endings = ['.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ', 'ï¼š', ':', 'å®Œæˆ', 'complete', 'ç»“æŸ', 'end']
            has_proper_ending = any(content.endswith(ending) for ending in proper_endings)

            # å¯¹äºåˆ—è¡¨ã€å‚æ•°ç­‰ç‰¹æ®Šæ ¼å¼ï¼Œä¸è¦æ±‚å¥å·ç»“å°¾
            import re
            list_patterns = [
                r'^\s*[-â€¢]\s',
                r'^\s*\d+\.\s',
                r'^\s*[a-zA-Z]\)\s',
                r':\s*$',
                r'\d+\s*(rpm|psi|Â°c|Â°f|kg|lb|ft|m|v|a)\s*$'
            ]

            is_special_format = any(re.search(pattern, content, re.IGNORECASE | re.MULTILINE) for pattern in list_patterns)

            if has_proper_ending or is_special_format:
                score += 0.3  # æœ‰é€‚å½“ç»“å°¾åŠ åˆ†
            else:
                score -= 0.2  # æ²¡æœ‰é€‚å½“ç»“å°¾æ‰£åˆ†

            # æ£€æŸ¥å¥å­å®Œæ•´æ€§
            sentences = re.split(r'[.ã€‚!ï¼?ï¼Ÿ]', content)
            complete_sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 3]

            if len(complete_sentences) > 0:
                score += 0.2  # æœ‰å®Œæ•´å¥å­åŠ åˆ†
            elif not is_special_format:
                score -= 0.3  # æ²¡æœ‰å®Œæ•´å¥å­ä¸”ä¸æ˜¯ç‰¹æ®Šæ ¼å¼æ‰£åˆ†

            # æ£€æŸ¥å†…å®¹çš„è¿è´¯æ€§
            if len(content) > 50:  # å¯¹è¾ƒé•¿å†…å®¹è¿›è¡Œè¿è´¯æ€§æ£€æŸ¥
                # æ£€æŸ¥æ˜¯å¦æœ‰çªç„¶çš„ä¸»é¢˜è½¬æ¢
                topic_keywords = {
                    'maintenance': ['ç»´ä¿®', 'æ£€æŸ¥', 'æ›´æ¢', 'å®‰è£…'],
                    'operation': ['æ“ä½œ', 'å¯åŠ¨', 'å…³é—­', 'è¿è¡Œ'],
                    'safety': ['å®‰å…¨', 'è­¦å‘Š', 'æ³¨æ„', 'å±é™©'],
                    'technical': ['å‚æ•°', 'è§„æ ¼', 'æ ‡å‡†', 'æŠ€æœ¯']
                }

                topic_counts = {}
                content_lower = content.lower()

                for topic, keywords in topic_keywords.items():
                    count = sum(1 for keyword in keywords if keyword in content_lower)
                    if count > 0:
                        topic_counts[topic] = count

                # å¦‚æœä¸»é¢˜è¿‡äºåˆ†æ•£ï¼Œæ‰£åˆ†
                if len(topic_counts) > 2:
                    score -= 0.1
                elif len(topic_counts) == 1:
                    score += 0.1  # ä¸»é¢˜é›†ä¸­åŠ åˆ†

            return max(0.0, min(1.0, score))

        except Exception as e:
            self.logger.warning(f"è¯­ä¹‰å®Œæ•´æ€§è¯„åˆ†è®¡ç®—å¤±è´¥: {e}")
            return 0.5
    
    def _calculate_information_density_score(self, chunk: TextChunk) -> float:
        """è®¡ç®—ä¿¡æ¯å¯†åº¦è¯„åˆ†"""
        try:
            score = 0.5  # ä»ä¸­ç­‰åŸºç¡€åˆ†å¼€å§‹
            content = chunk.content

            total_chars = len(content)
            if total_chars == 0:
                return 0.0

            # è®¡ç®—æœ‰æ•ˆå­—ç¬¦æ¯”ä¾‹
            non_space_chars = len(content.replace(' ', '').replace('\n', '').replace('\t', '').replace('\r', ''))
            content_ratio = non_space_chars / total_chars

            if content_ratio >= 0.8:
                score += 0.3  # é«˜å¯†åº¦å†…å®¹åŠ åˆ†
            elif content_ratio >= 0.7:
                score += 0.2
            elif content_ratio >= 0.6:
                score += 0.1
            elif content_ratio < 0.5:
                score -= 0.4  # ä½å¯†åº¦å†…å®¹ä¸¥é‡æ‰£åˆ†
            elif content_ratio < 0.6:
                score -= 0.2

            # è®¡ç®—ä¿¡æ¯å…³é”®è¯å¯†åº¦
            info_keywords = [
                'å‚æ•°', 'æ•°å€¼', 'è§„æ ¼', 'æ ‡å‡†', 'è¦æ±‚', 'æ­¥éª¤', 'æ–¹æ³•', 'ç¨‹åº',
                'æ£€æŸ¥', 'æµ‹è¯•', 'ç»´ä¿®', 'æ›´æ¢', 'å®‰è£…', 'è°ƒæ•´', 'æ ¡å‡†',
                'parameter', 'value', 'specification', 'standard', 'requirement',
                'step', 'method', 'procedure', 'check', 'test', 'maintenance'
            ]

            content_lower = content.lower()
            keyword_count = sum(1 for keyword in info_keywords if keyword in content_lower)
            keyword_density = keyword_count / max(1, len(content.split()))

            if keyword_density >= 0.2:
                score += 0.3  # é«˜å…³é”®è¯å¯†åº¦åŠ åˆ†
            elif keyword_density >= 0.1:
                score += 0.2
            elif keyword_density >= 0.05:
                score += 0.1
            else:
                score -= 0.2  # ä½å…³é”®è¯å¯†åº¦æ‰£åˆ†

            # æ£€æŸ¥æ•°å­—å’ŒæŠ€æœ¯æ•°æ®çš„å¯†åº¦
            import re
            numbers = re.findall(r'\d+(?:\.\d+)?', content)
            units = re.findall(r'\d+\s*(rpm|psi|Â°c|Â°f|kg|lb|ft|m|v|a|bar|mpa)', content, re.IGNORECASE)

            if len(numbers) > 0:
                number_density = len(numbers) / max(1, len(content.split()))
                if number_density > 0.2:
                    score += 0.2  # é«˜æ•°å€¼å¯†åº¦åŠ åˆ†
                elif number_density > 0.1:
                    score += 0.1

            if len(units) > 0:
                score += 0.1  # åŒ…å«æŠ€æœ¯å•ä½åŠ åˆ†

            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ— æ„ä¹‰çš„é‡å¤å†…å®¹
            words = content.split()
            if len(words) > 5:
                unique_words = set(words)
                repetition_ratio = len(words) / len(unique_words)
                if repetition_ratio > 3:
                    score -= 0.3  # é‡å¤åº¦å¤ªé«˜æ‰£åˆ†
                elif repetition_ratio < 1.5:
                    score += 0.1  # è¯æ±‡ä¸°å¯Œåº¦é«˜åŠ åˆ†

            return max(0.0, min(1.0, score))

        except Exception as e:
            self.logger.warning(f"ä¿¡æ¯å¯†åº¦è¯„åˆ†è®¡ç®—å¤±è´¥: {e}")
            return 0.5
    
    def _calculate_structure_quality_score(self, chunk: TextChunk) -> float:
        """è®¡ç®—ç»“æ„è´¨é‡è¯„åˆ†"""
        try:
            score = 0.4  # ä»è¾ƒä½çš„åŸºç¡€åˆ†å¼€å§‹
            content = chunk.content

            # æ£€æŸ¥æ ‡é¢˜å’Œç« èŠ‚ç»“æ„
            import re
            structure_markers = [
                r'^ç¬¬\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+\s*[ç« èŠ‚æ¡]',
                r'^Chapter\s+\d+',
                r'^Section\s+\d+',
                r'^#{1,6}\s',
                r'^\d+\.\d+',
                r'^[A-Z][A-Z\s]+:$'
            ]

            has_structure = any(re.search(pattern, content, re.IGNORECASE | re.MULTILINE) for pattern in structure_markers)

            if has_structure:
                score += 0.4  # æœ‰æ˜ç¡®ç»“æ„æ ‡è®°å¤§å¹…åŠ åˆ†

            # æ£€æŸ¥åˆ—è¡¨ç»“æ„
            list_patterns = [
                r'^\s*[-â€¢]\s',
                r'^\s*\d+\.\s',
                r'^\s*[a-zA-Z]\)\s',
                r'^\s*\([a-zA-Z0-9]+\)\s'
            ]

            list_items = []
            for pattern in list_patterns:
                matches = re.findall(pattern, content, re.MULTILINE)
                list_items.extend(matches)

            if len(list_items) > 1:
                score += 0.3  # æœ‰åˆ—è¡¨ç»“æ„åŠ åˆ†

                # æ£€æŸ¥åˆ—è¡¨çš„ä¸€è‡´æ€§
                if len(set(list_items)) == len(list_items):
                    score += 0.1  # åˆ—è¡¨æ ‡è®°ä¸€è‡´åŠ åˆ†
            elif len(list_items) == 1:
                score += 0.1  # æœ‰å•ä¸ªåˆ—è¡¨é¡¹å°å¹…åŠ åˆ†

            # æ£€æŸ¥æ®µè½ç»“æ„
            paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
            if len(paragraphs) > 1:
                score += 0.2  # æœ‰å¤šæ®µè½ç»“æ„åŠ åˆ†

            # æ£€æŸ¥ç‰¹æ®Šç»“æ„ï¼ˆè¡¨æ ¼ã€ä»£ç å—ç­‰ï¼‰
            special_structures = [
                r'\|.*\|',  # è¡¨æ ¼
                r'```.*```',  # ä»£ç å—
                r'^\s*\w+[:ï¼š]\s*\w+',  # é”®å€¼å¯¹
                r'\d+\s*[xÃ—]\s*\d+',  # å°ºå¯¸è§„æ ¼
            ]

            for pattern in special_structures:
                if re.search(pattern, content, re.MULTILINE | re.DOTALL):
                    score += 0.2  # æœ‰ç‰¹æ®Šç»“æ„åŠ åˆ†
                    break

            # æ£€æŸ¥ç»“æ„çš„å®Œæ•´æ€§
            incomplete_patterns = [
                (r'^\s*æ­¥éª¤\s*\d+', r'å®Œæˆ|ç»“æŸ|end|complete'),
                (r'^\s*æ³¨æ„[:ï¼š]', r'[.ã€‚!ï¼]$'),
                (r'^\s*è­¦å‘Š[:ï¼š]', r'[.ã€‚!ï¼]$'),
            ]

            for start_pattern, end_pattern in incomplete_patterns:
                if re.search(start_pattern, content, re.IGNORECASE | re.MULTILINE):
                    if not re.search(end_pattern, content, re.IGNORECASE | re.MULTILINE):
                        score -= 0.3  # ç»“æ„ä¸å®Œæ•´æ‰£åˆ†
                        break

            return max(0.0, min(1.0, score))

        except Exception as e:
            self.logger.warning(f"ç»“æ„è´¨é‡è¯„åˆ†è®¡ç®—å¤±è´¥: {e}")
            return 0.4
    
    def _calculate_size_appropriateness_score(self, chunk: TextChunk) -> float:
        """è®¡ç®—å¤§å°é€‚å½“æ€§è¯„åˆ†"""
        try:
            char_count = chunk.character_count

            optimal_min = self.chunk_size * 0.8
            optimal_max = self.chunk_size * 1.2

            if optimal_min <= char_count <= optimal_max:
                return 1.0

            if char_count < optimal_min:
                if char_count < self.min_chunk_size:
                    # å¯¹è¿‡å°çš„åˆ†å—æ›´ä¸¥æ ¼è¯„åˆ†
                    ratio = char_count / self.min_chunk_size
                    return max(0.0, ratio * 0.3)  # é™ä½åŸºç¡€åˆ†æ•°
                else:
                    # å°äºæœ€ä¼˜ä½†åœ¨å¯æ¥å—èŒƒå›´å†…
                    ratio = char_count / optimal_min
                    return 0.3 + ratio * 0.4  # è°ƒæ•´è¯„åˆ†èŒƒå›´
            else:
                if char_count > self.max_chunk_size:
                    ratio = self.max_chunk_size / char_count
                    return max(0.0, ratio * 0.5)
                else:
                    ratio = optimal_max / char_count
                    return 0.5 + ratio * 0.5

        except Exception as e:
            self.logger.warning(f"å¤§å°é€‚å½“æ€§è¯„åˆ†è®¡ç®—å¤±è´¥: {e}")
            return 0.5
    
    def calculate_chunk_quality(self, chunk: TextChunk) -> float:
        """è®¡ç®—åˆ†å—è´¨é‡è¯„åˆ†ï¼ˆèˆªç©ºRAGç³»ç»Ÿä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            if not chunk.content.strip():
                return 0.0

            if chunk.character_count < 10:
                return 0.1

            # æ ¹æ®æ–‡æ¡£ç±»å‹è·å–æƒé‡é…ç½®
            weights = self._get_quality_weights(chunk.metadata)

            # è®¡ç®—å„ç»´åº¦è¯„åˆ†
            aviation_score = self._calculate_aviation_specific_score(chunk)
            semantic_score = self._calculate_semantic_completeness_score(chunk)
            density_score = self._calculate_information_density_score(chunk)
            structure_score = self._calculate_structure_quality_score(chunk)
            size_score = self._calculate_size_appropriateness_score(chunk)

            # å¯é€‰çš„è°ƒè¯•è¾“å‡ºï¼ˆæ³¨é‡Šæ‰ä»¥ç®€åŒ–è¾“å‡ºï¼‰
            # print(f"   [è°ƒè¯•] èˆªç©ºç‰¹å®šæ€§: {aviation_score:.3f}")
            # print(f"   [è°ƒè¯•] è¯­ä¹‰å®Œæ•´æ€§: {semantic_score:.3f}")
            # print(f"   [è°ƒè¯•] ä¿¡æ¯å¯†åº¦: {density_score:.3f}")
            # print(f"   [è°ƒè¯•] ç»“æ„è´¨é‡: {structure_score:.3f}")
            # print(f"   [è°ƒè¯•] å¤§å°é€‚å½“æ€§: {size_score:.3f}")

            # åŠ æƒè®¡ç®—æ€»åˆ†
            total_score = (
                aviation_score * weights['aviation_specific'] +
                semantic_score * weights['semantic_completeness'] +
                density_score * weights['information_density'] +
                structure_score * weights['structure_quality'] +
                size_score * weights['size_appropriateness']
            )

            # å¯¹äºæ˜æ˜¾æœ‰é—®é¢˜çš„å†…å®¹ï¼Œåº”ç”¨æƒ©ç½šæœºåˆ¶
            penalty = 0.0

            # å†…å®¹è¿‡çŸ­æƒ©ç½š
            if chunk.character_count < 30:
                penalty += 0.4
            elif chunk.character_count < 50:
                penalty += 0.2

            # ç©ºç™½å­—ç¬¦è¿‡å¤šæƒ©ç½š
            non_space_ratio = len(chunk.content.replace(' ', '').replace('\n', '').replace('\t', '')) / len(chunk.content)
            if non_space_ratio < 0.3:
                penalty += 0.5
            elif non_space_ratio < 0.5:
                penalty += 0.3
            elif non_space_ratio < 0.6:
                penalty += 0.1

            # åº”ç”¨æƒ©ç½šï¼Œä½†ä¿ç•™æœ€ä½åˆ†æ•°
            final_score = max(0.1, total_score - penalty)

            return round(min(1.0, final_score), 3)

        except Exception as e:
            self.logger.warning(f"åˆ†å—è´¨é‡è¯„åˆ†è®¡ç®—å¤±è´¥: {e}")
            return 0.5


def create_test_chunks():
    """åˆ›å»ºæµ‹è¯•ç”¨çš„åˆ†å—æ•°æ®"""
    
    test_cases = [
        {
            'name': 'å®Œæ•´çš„ç»´ä¿®æ­¥éª¤',
            'content': '''ç¬¬3ç«  å‘åŠ¨æœºç»´ä¿®ç¨‹åº
3.1 æ—¥å¸¸æ£€æŸ¥æ­¥éª¤
è­¦å‘Šï¼šæ£€æŸ¥å‰å¿…é¡»å…³é—­å‘åŠ¨æœºå¹¶ç­‰å¾…å†·å´ã€‚
æ­¥éª¤1ï¼šæ£€æŸ¥å‘åŠ¨æœºå¤–è§‚ï¼ŒæŸ¥çœ‹æ˜¯å¦æœ‰æ³„æ¼æˆ–æŸåã€‚
æ­¥éª¤2ï¼šæ£€æŸ¥æœºæ²¹æ¶²ä½ï¼Œç¡®ä¿åœ¨æ­£å¸¸èŒƒå›´å†…ï¼ˆ2.5-3.0å‡ï¼‰ã€‚
æ­¥éª¤3ï¼šæ£€æŸ¥å†·å´æ¶²æ¸©åº¦ï¼Œæ­£å¸¸å·¥ä½œæ¸©åº¦åº”ä¸º85-95Â°Cã€‚
æ£€æŸ¥å®Œæˆåï¼Œè®°å½•æ‰€æœ‰å‚æ•°å¹¶ç­¾å­—ç¡®è®¤ã€‚''',
            'chunk_type': ChunkType.MAINTENANCE_MANUAL,
            'expected_score_range': (0.8, 1.0)
        },
        
        {
            'name': 'ä¸å®Œæ•´çš„å®‰å…¨è­¦å‘Š',
            'content': '''è­¦å‘Šï¼šåœ¨è¿›è¡Œæ¶²å‹ç³»ç»Ÿç»´ä¿®æ—¶ï¼Œå¿…é¡»æ³¨æ„
å‹åŠ›é‡Šæ”¾ç¨‹åºåŒ…æ‹¬ï¼š
1. å…³é—­ä¸»ç”µæº
2. é‡Šæ”¾ç³»ç»Ÿå‹åŠ›''',
            'chunk_type': ChunkType.MAINTENANCE_MANUAL,
            'expected_score_range': (0.4, 0.7)
        },
        
        {
            'name': 'æŠ€æœ¯å‚æ•°åˆ—è¡¨',
            'content': '''æ¶²å‹ç³»ç»ŸæŠ€æœ¯è§„æ ¼ï¼š
å·¥ä½œå‹åŠ›ï¼š3000 PSI
æœ€å¤§å‹åŠ›ï¼š3500 PSI
å·¥ä½œæ¸©åº¦ï¼š-40Â°C åˆ° +85Â°C
æ¶²å‹æ²¹ç±»å‹ï¼šMIL-H-5606
æ²¹ç®±å®¹é‡ï¼š15å‡
è¿‡æ»¤å™¨è§„æ ¼ï¼š25å¾®ç±³''',
            'chunk_type': ChunkType.TECHNICAL_STANDARD,
            'expected_score_range': (0.6, 0.8)  # è°ƒæ•´é¢„æœŸèŒƒå›´ï¼Œå› ä¸ºå†…å®¹è¾ƒçŸ­
        },

        {
            'name': 'æˆªæ–­çš„èˆªç©ºæœ¯è¯­',
            'content': '''æ¶²å‹ç³»ç»Ÿæ£€æŸ¥ç¨‹åº
æ£€æŸ¥æ¶²å‹æ³µçš„å·¥ä½œçŠ¶æ€ï¼Œç¡®ä¿å‹åŠ›ç¨³å®šã€‚å¦‚æœå‘ç°æ¶²å‹
æ²¹æ³„æ¼ï¼Œåº”ç«‹å³åœæ­¢æ“ä½œå¹¶è¿›è¡Œç»´ä¿®ã€‚æ£€æŸ¥å®Œæˆåæ›´æ–°ç»´ä¿®è®°å½•ã€‚''',
            'chunk_type': ChunkType.MAINTENANCE_MANUAL,
            'expected_score_range': (0.3, 0.6)
        },

        {
            'name': 'å®Œæ•´çš„èˆªç©ºæ³•è§„',
            'content': '''ç¬¬147æ¡ èˆªç©ºå™¨ç»´ä¿®äººå‘˜èµ„è´¨è¦æ±‚
147.1 åŸºæœ¬è¦æ±‚
æŒè¯ç»´ä¿®äººå‘˜å¿…é¡»å…·å¤‡ä»¥ä¸‹æ¡ä»¶ï¼š
(a) å¹´æ»¡18å‘¨å²ï¼›
(b) å…·æœ‰ç›¸åº”çš„æŠ€æœ¯åŸ¹è®­ç»å†ï¼›
(c) é€šè¿‡ç†è®ºå’Œå®è·µè€ƒè¯•ï¼›
(d) èº«ä½“å¥åº·ï¼Œèƒ½å¤Ÿèƒœä»»ç»´ä¿®å·¥ä½œã€‚
æœ¬æ¡æ¬¾è‡ªå‘å¸ƒä¹‹æ—¥èµ·ç”Ÿæ•ˆï¼Œæ‰€æœ‰ç»´ä¿®äººå‘˜å¿…é¡»ä¸¥æ ¼éµå®ˆã€‚''',
            'chunk_type': ChunkType.REGULATION,
            'expected_score_range': (0.8, 1.0)
        },

        {
            'name': 'ç©ºç™½å†…å®¹è¿‡å¤š',
            'content': '''


æ£€æŸ¥     é¡¹ç›®ï¼š     å‘åŠ¨æœº


çŠ¶æ€ï¼š     æ­£å¸¸



''',
            'chunk_type': ChunkType.MAINTENANCE_MANUAL,
            'expected_score_range': (0.1, 0.4)
        }
    ]
    
    chunks = []
    for i, case in enumerate(test_cases):
        metadata = ChunkMetadata(
            chunk_id=f"test_chunk_{i}",
            chunk_type=case['chunk_type'],
            source_document=f"test_doc_{case['name']}"
        )
        
        chunk = TextChunk(
            content=case['content'],
            metadata=metadata,
            word_count=len(case['content'].split()),
            character_count=len(case['content'])
        )
        
        chunks.append({
            'chunk': chunk,
            'name': case['name'],
            'expected_range': case['expected_score_range']
        })
    
    return chunks


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ èˆªç©ºRAGç³»ç»Ÿåˆ†å—è´¨é‡è¯„ä¼°æ”¹è¿›æ•ˆæœæµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºè´¨é‡è¯„ä¼°å¼•æ“
    config = {
        'chunk_size': 1000,
        'min_chunk_size': 100,
        'max_chunk_size': 2000
    }
    
    engine = QualityAssessmentEngine(config)
    
    # è·å–æµ‹è¯•æ•°æ®
    test_chunks = create_test_chunks()
    
    print(f"\nğŸ“Š æµ‹è¯•ç”¨ä¾‹æ€»æ•°: {len(test_chunks)}")
    print("-" * 60)
    
    results = []
    
    for test_case in test_chunks:
        chunk = test_case['chunk']
        name = test_case['name']
        expected_range = test_case['expected_range']
        
        # è®¡ç®—è´¨é‡è¯„åˆ†
        quality_score = engine.calculate_chunk_quality(chunk)
        
        # æ£€æŸ¥æ˜¯å¦åœ¨é¢„æœŸèŒƒå›´å†…
        in_range = expected_range[0] <= quality_score <= expected_range[1]
        status = "âœ… é€šè¿‡" if in_range else "âŒ æœªé€šè¿‡"
        
        print(f"\nğŸ“ æµ‹è¯•ç”¨ä¾‹: {name}")
        print(f"   å†…å®¹é•¿åº¦: {chunk.character_count} å­—ç¬¦")
        print(f"   æ–‡æ¡£ç±»å‹: {chunk.metadata.chunk_type}")
        print(f"   è´¨é‡è¯„åˆ†: {quality_score:.3f}")
        print(f"   é¢„æœŸèŒƒå›´: {expected_range[0]:.1f} - {expected_range[1]:.1f}")
        print(f"   æµ‹è¯•ç»“æœ: {status}")
        
        # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
        preview = chunk.content[:100].replace('\n', ' ')
        if len(chunk.content) > 100:
            preview += "..."
        print(f"   å†…å®¹é¢„è§ˆ: {preview}")
        
        results.append({
            'name': name,
            'score': quality_score,
            'expected': expected_range,
            'passed': in_range
        })
    
    # ç»Ÿè®¡ç»“æœ
    passed_count = sum(1 for r in results if r['passed'])
    total_count = len(results)
    pass_rate = passed_count / total_count * 100
    
    print("\n" + "=" * 60)
    print("ğŸ“ˆ æµ‹è¯•ç»“æœç»Ÿè®¡")
    print("-" * 60)
    print(f"é€šè¿‡æµ‹è¯•: {passed_count}/{total_count} ({pass_rate:.1f}%)")
    
    if pass_rate >= 80:
        print("ğŸ‰ è´¨é‡è¯„ä¼°æ”¹è¿›æ•ˆæœè‰¯å¥½ï¼")
    elif pass_rate >= 60:
        print("âš ï¸  è´¨é‡è¯„ä¼°æœ‰æ‰€æ”¹è¿›ï¼Œä½†ä»éœ€ä¼˜åŒ–")
    else:
        print("âŒ è´¨é‡è¯„ä¼°éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›")
    
    print("\nâœ¨ æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    main()
